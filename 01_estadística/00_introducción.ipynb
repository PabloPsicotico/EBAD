{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "* [1. Estadística y análisis de datos.](#1.-Estadística-y-análisis-de-datos.)\n",
    "\t* [1.1 Introducción](#1.1-Introducción)\n",
    "\t* [1.2 Estadística](#1.2-Estadística)\n",
    "\t* [1.3 Análisis exploratorio de datos](#1.3-Análisis-exploratorio-de-datos)\n",
    "\t\t* [1.3.1 Centralidad](#1.3.1-Centralidad)\n",
    "\t\t\t* [1.3.1.1 Media](#1.3.1.1-Media)\n",
    "\t\t\t* [1.3.1.2 Mediana](#1.3.1.2-Mediana)\n",
    "\t\t\t* [1.3.1.3 Moda](#1.3.1.3-Moda)\n",
    "\t\t* [1.3.2 Dispersión](#1.3.2-Dispersión)\n",
    "\t\t\t* [1.3.2.1 Varianza](#1.3.2.1-Varianza)\n",
    "\t\t\t* [1.3.2.2 Desviación estándar](#1.3.2.2-Desviación-estándar)\n",
    "\t\t* [1.3.3 Cuantil](#1.3.3-Cuantil)\n",
    "\t\t* [1.3.4 Histogramas](#1.3.4-Histogramas)\n",
    "\t\t* [1.3.5 Kernel Density plot](#1.3.5-Kernel-Density-plot)\n",
    "\t\t* [1.3.6 Gráficos de cajas o de bigotes (Box plot o Wisker-plot)](#1.3.6-Gráficos-de-cajas-o-de-bigotes-%28Box-plot-o-Wisker-plot%29)\n",
    "\t\t* [1.3.7 Gráficos de violín (violin plot)](#1.3.7-Gráficos-de-violín-%28violin-plot%29)\n",
    "\t\t* [1.3.8 Datos aberrantes](#1.3.8-Datos-aberrantes)\n",
    "\t* [1.4 Relación entre dos variables](#1.4-Relación-entre-dos-variables)\n",
    "\t\t* [1.4.1 Gráfico de dispersión](#1.4.1-Gráfico-de-dispersión)\n",
    "\t\t* [1.4.2 Correlación](#1.4.2-Correlación)\n",
    "\t\t* [1.4.3 Correlación y causalidad](#1.4.3-Correlación-y-causalidad)\n",
    "\t* [1.5 Estadística Inferencial](#1.5-Estadística-Inferencial)\n",
    "\t\t* [1.5.1 Los tres objetivos de la inferencia estadística](#1.5.1-Los-tres-objetivos-de-la-inferencia-estadística)\n",
    "\t* [1.6 Probabilidad](#1.6-Probabilidad)\n",
    "\t\t* [1.6.1 Probabilidad clásica](#1.6.1-Probabilidad-clásica)\n",
    "\t\t* [1.6.2 Probabilidad frecuentista](#1.6.2-Probabilidad-frecuentista)\n",
    "\t\t* [1.6.3 Probabilidad Bayesiana](#1.6.3-Probabilidad-Bayesiana)\n",
    "\t\t* [1.6.4 Distribuciones de probabilidad](#1.6.4-Distribuciones-de-probabilidad)\n",
    "\t\t* [1.6.5 Incerteza](#1.6.5-Incerteza)\n",
    "\t\t* [1.6.6 Variables aleatorias](#1.6.6-Variables-aleatorias)\n",
    "\t\t* [1.6.7 Distribuciones de probabilidad comunes](#1.6.7-Distribuciones-de-probabilidad-comunes)\n",
    "\t\t\t* [1.6.7.1 Distribución uniforme](#1.6.7.1-Distribución-uniforme)\n",
    "\t\t\t* [1.6.7.2 Distribución Gaussiana (o normal)](#1.6.7.2-Distribución-Gaussiana-%28o-normal%29)\n",
    "\t\t\t* [1.6.7.3 Distribución t de Student](#1.6.7.3-Distribución-t-de-Student)\n",
    "\t\t\t* [1.6.7.4 Distribución exponencial](#1.6.7.4-Distribución-exponencial)\n",
    "\t\t\t* [1.6.7.5 Distribución beta](#1.6.7.5-Distribución-beta)\n",
    "\t\t\t* [1.6.7.6 Distribución Gamma](#1.6.7.6-Distribución-Gamma)\n",
    "\t\t\t* [1.6.7.7 Distribución binomial](#1.6.7.7-Distribución-binomial)\n",
    "\t\t\t* [1.6.7.8 Distribución de Poisson](#1.6.7.8-Distribución-de-Poisson)\n",
    "\t* [1.7 Probability plot](#1.7-Probability-plot)\n",
    "\t* [1.8 ¿Por qué Normal?](#1.8-¿Por-qué-Normal?)\n",
    "\t* [1.9 El teorema del límite central](#1.9-El-teorema-del-límite-central)\n",
    "\t* [1.10 La ley de los grandes números (el casino siempre gana).](#1.10-La-ley-de-los-grandes-números-%28el-casino-siempre-gana%29.)\n",
    "\t* [1.11 Z-score](#1.11-Z-score)\n",
    "\t* [1.12 Error estándard](#1.12-Error-estándard)\n",
    "\t* [1.13 Para seguir leyendo](#1.13-Para-seguir-leyendo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/matplotlib/__init__.py:872: UserWarning: axes.color_cycle is deprecated and replaced with axes.prop_cycle; please use the latter.\n",
      "  warnings.warn(self.msg_depr % (key, alt_key))\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_color_codes()  # Fija los nombres cortos para los colores según la paleta de seaborn\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Estadística y análisis de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El objetivo de este primer capítulo es introducir algunos términos y conceptos generales de estadística, que nos serán de utilidad para discutir todos los tópicos de este curso. La introducción es bastante general, por lo que a lo largo del texto se proveen de enlaces para seguir leyendo y profundizando en el tema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Estadística"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La [estadística](https://es.wikipedia.org/wiki/Estad%C3%ADstica) es el estudio de la recolección, anális, interpretación y organización de datos. El corolario de esta definición podría ser que además de ser una disciplina científica en si misma, la estadística es una disciplina auxiliar de todas las demás ciencias.\n",
    "\n",
    "En los últimos años ha comenzado a emerger una disciplina llamada ciencia de datos (_data science_ en inglés), para muchos no es más que un nuevo y [sexy](https://hbr.org/2012/10/data-scientist-the-sexiest-job-of-the-21st-century/) nombre para llamar a la vieja estadística, algo así como una campaña mediática de mejoramiento de imágen. Para otros la ciencia de datos es una disciplina emparentada con la estadística, pero que contiene realmente nuevos métodos y objetivos.\n",
    "\n",
    "Una posible diferencia entre la ciencia de datos y la estadística se muestra en el siguiente diagrama de Venn. \n",
    "<a href=\"http://drewconway.com/zia/2013/3/26/the-data-science-venn-diagram\">\n",
    "<img src='imagenes/DS.png' width=300 >\n",
    "</a>\n",
    "\n",
    "Seún el diagrama, la estadística es una de las partes de la ciencia de datos. La gran diferencia entre la investigación tradicional y la ciencia de datos radica no en los conocimientos estadísticos (que ambas requieren) si no en las habilidades de _hacking_. En esto contexto _hacking_ no hace referencia a la capacidad de vulnerar la seguridad de computadoras ajenas, si no a _la capacidad técnica y creatividad para encontrar soluciones mediante el uso de código_, es decir la capacidad de programar. Es requisito de este curso tener algunas nociones básicas de programación. El lenguaje que usaremos en este curso es Python. Si no sabés programar o no sabes Python o simplemente estás un poco oxidada entonces es buena idea dejar de leer esto por un momento y pasar a leer la siguiente [notebook](https://github.com/aloctavodia/EBAD/tree/master/00_intro_python) y las referencias allí indicadas. Uno de los objetivos del curso es aplicar esos conocimientos de programación para el análisis de datos. Otro de los objetivos del curso es evitar que caigan en la zona peligrosa! Para lo cual deberán aprender a usar (y crear) modelos estadísticos, interpretarlos y comprender cuales son los límites de las aproximaciones usadas.\n",
    "\n",
    "Como puede verse en el diagrama, tanto la investigación tradicional como la ciencia de datos necesitan ir acompañadas de _conocimiento específico de dominio_. La estadística puede ser de ayuda para estudiar genomas o partículas elementales, pero para poder hacer preguntas relevantes (y entender las respuestas) primero hay que comprender que son los genomas y que las partículas elementales. La estadística NO es una máquina auto-mágica por donde entran datos en crudo por un lado y sale información por el otro. La estadística es una herramienta que nos ayuda a pensar y tomar decisiones de forma adecuada, pero requiere del conocimiento, el criterio y la responsabilidad de quien la usa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Análisis exploratorio de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo primero que se suele hacer al enfrentarse a un conjunto nuevo de datos, es intentar ganar intuición sobre los datos que tenemos enfrente. Para ello se han desarrollado una colección de métricas y métodos colectivamente llamados [Análisis exploratorio de datos](https://en.wikipedia.org/wiki/Exploratory_data_analysis) (EDA por sus siglas en inglés), la cual se compone basicamente de dos herramientas complementarias:\n",
    "\n",
    "1. La vizualización de datos\n",
    "1. La estadística descriptiva. \n",
    "\n",
    "Esta última se ocupa de describir de forma cuantitativa un conjunto de datos. Para ello se recurre a un conjunto de medidas que nos ayudan a resumir los datos en unos pocos números. En general se habla de medidas de centralidad, de dispersión y con menor frecuencia se suele hablar de medidas de forma."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1 Centralidad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.1.1 Media"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También llamada media aritmética o valor esperado,  es igual a la suma de todos los valores de una variable pesados por la probabilidad de cada uno de esos valores. A continuación se describen varias notaciones matemáticas para este concepto.\n",
    "\n",
    "$$E[X] = \\int_{-\\infty}^{\\infty} x d(x) = \\frac{1}{n} \\sum_{i=1}^n{x_i}\n",
    " = \\sum x P(x) $$\n",
    "\n",
    "El primer término es usado comunmente en estadística y probabilidad como una notación breve para indicar un promedio. El segundo término muestra que la media es la integral (área bajo la curva) de una distribución. El tercer término es quizá la forma más familiar para ustedes y el cuarto termino refleja claramente la definición dada en el párrafo anterior.\n",
    "\n",
    "A veces el término promedio se usa como sinónimo de media, pero hay autores que prefieren llamar promedio a cualquier medida que sirva para calcular la centralidad de una distribución.\n",
    "\n",
    "La media es una buena descripción si los objetos que estamos midiendo son más o menos similares, pero puede ofrecer una visión muy distorsionada si la diferencia entre objetos medidos es grande, como suele suceder por ejemplo con los salarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.1.2 Mediana"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La mediana es el número que separa un conjunto de datos en una mitad superior y otra inferior. Como veremos más adelante es una medida más robusta que la media a valores extremos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.1.3 Moda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El valor más frecuente de una distribución.\n",
    "\n",
    "\n",
    "Veamos como lucen estas tres medidas para un conjunto de datos sintéticos (inventados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe0AAAFVCAYAAADCLbfjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3XlcVXX+x/E32wXZ3CIxEFxCzS0LxuhnmZULppW5lJo0\nPlxS0yZlFHfRtDDcci3NpcIas1GzRXMZR22cmsxGzRgtBURIDYkUroP3Kuf3B9M1BPWqV+Hg6/l4\n9AjO8rnf+0l737Pc73EzDMMQAAAo99zLegAAAMA5hDYAACZBaAMAYBKENgAAJkFoAwBgEoQ2AAAm\n4Xm5lefOndPYsWOVlZUlu92uQYMG6c4779To0aPl7u6uiIgIJSQkSJJWrVqlDz74QF5eXho0aJBa\nt259M8YPAMAt47Kh/fHHH6tq1apKSkrS6dOn9eSTT6phw4aKi4tTVFSUEhIStGXLFjVv3lzJycla\nu3atCgoK1LNnT7Vs2VJeXl43630AAFDhXTa0O3TooJiYGEnS+fPn5eHhoZSUFEVFRUmSWrVqpZ07\nd8rd3V2RkZHy9PSUv7+/ateurYMHD6pJkyY3/h0AAHCLuOw17UqVKsnX11f5+fl66aWXNHz4cP1+\nAjU/Pz/l5+fLarUqICDAsdzX11d5eXk3btQAANyCrngj2rFjx/THP/5RTz31lDp27Ch39wu7WK1W\nBQYGyt/fX/n5+SWWXwkzqAIA4LzLnh4/efKk+vXrp4kTJyo6OlqSdNddd2nXrl36wx/+oB07dig6\nOlpNmzbV7NmzZbPZdPbsWaWmpioiIuKKL+7m5qbsbI7IryQoKIA+OYleOYc+OY9eOYc+OScoKODK\nG13GZUN70aJFOn36tBYuXKgFCxbIzc1N48aN09SpU2W321WvXj3FxMTIzc1NsbGx6tWrlwzDUFxc\nnCwWy3UNDAAAFOdW1k/54pPZlfEJ1nn0yjn0yXn0yjn0yTnXe6TN5CoAAJgEoQ0AgEkQ2gAAmASh\nDQCASVz27nGzOX/+vDIzM1xaMzQ0TB4eHi6tCQDAtahQoZ2ZmaE/z/pMFr/qLqlns+ZoZlxHhYfX\ncUk9AACuR4UKbUmy+FWXT2CNsh4GAAAuxzVtAABMgtAGAMAkCG0AqGCysjI1atRwdejwiLp06aj5\n81+XzWZzrD9+/LiGDx+itm0fVO/eT+urr/7pVN29e/+tVq1a6Pjx445lZ86cUWLiy4qOjtbjj7dT\nUtIrKigocPl7csY77yzViy8OlCRt2PCpunTpWCbjuJEIbQCoQM6dO6dRo4bL29tHixYtV0LCVH3x\nxTa99dYbjm1Gj45T1arVtGRJsmJiHtP48fE6fvzYZevabDZNmzalxPKFC+fowIEULV26VNOnz9G+\nfXv1xhtzXf6+nOXm5iZJevTRdlq+/L0yG8eNQmgDQAWSkrJfP/2UpXHjEhQWFq67775H/fsP0qZN\nGyRJu3fvUmZmhuLjxyk8vLZ69+6jJk2a6dNP11227pIlb6p69dtKLN+799/q1KmzGjdurIYN79JT\nT3XV7t3f3JD3djUsFosqV65S1sNwOUIbACqQsLDamj59jry9fYotz88vephHSsp+RUQ0kI/PhfXN\nmjXX/v37LlnzwIEUbd78uYYMeUkXP2Pqrrsaa/v2rTp16pTy8vK0ffvf1bDhXaXW2bDhU73wQn8l\nJy9Xhw6P6MknY7R58+faunWLunbtpA4dHtGiRQsc29vtds2ZM1OdOrVVx46PauLEMcrN/cWx/siR\ndA0d+rzatHlAf/rTIOXm5jrWrV//SbHT4zt3fqG+fXvrkUdaKiamtRISxujMmTOXa2W5RGgDQAVS\npUoVRUb+wfG7YRhas2aV/vCH+yRJOTknddttQcX2qVatmrKzfy613rlz5zRt2lQNHTpcgYGVS6wf\nNmykCgoKFB0drY4dH9WpU6cUFxd/yfEdOJCio0cztGTJu3r44TZKSnpVa9as0vTpczRw4BCtWPG2\nUlMPS5LefHO+UlL2a8aMOZo//y0ZhqH4+GGSigJ9xIiXVKNGsJYtW6G2bWO0bt1qx+v8dppcko4d\n+0kTJoxSly7d9P77qzVlymv69tvd+uij1TKbCvc9bQC4kX7ZuEE5H6+Tcfbm3Wzl5u2j6k88qWrt\nO1z1vnPnztKhQz9qyZJkSVJBQYEsFq9i23h5WWSz2UvdPzl5uYKDg/Xoo22VlZVZLAwlafbsJBUU\nFGj58uXKz7dp3rzZmjIlQYmJM0qtV1hYqLi4UfLx8dETT3TWmjWr1L//INWtW09169bTm2/OU0ZG\nukJCQrR27YdatOhtRUTUlySNHz9JHTu20b59e2S15uvUqV81cuRY+fj4KCystvbs2a3s7OwSr3n+\n/Hm99NIIderUWZIUHBysyMg/KD099eqaWQ4Q2gBwFXI3bbypgS1JxtkC5W7aeNWh/frrM7Ru3WpN\nnZqk8PDakoqu9Vqt1mLb2e02+fh4l9g/NfWwVq/+QMuXv180jotOjZ88ma2NG9dr4cKlio6OVnZ2\nnqZMmaann35SBw8eUIMGDUvUrFy5iuPU/G+n8GvUCHas9/b2ls1mV1ZWlux2u4YMGVDsde12u44e\nzdCpU6cUEhJS7DR/gwaNlJ29vcRrhobWkpeXl959d5lSUw8rLS1V6empatOm/WX7Vx4R2gBwFaq2\na18mR9pV2zkfMIZhKDHxZW3ZslEvv5yoli0fdKwLCrpdhw8fKrZ9Tk5OqTeZbd++Vfn5+erVq9tv\nlWUYhmJjn1Z8/FiFhtaSJN15Z4Rjn5o171BAQKB++imz1NAu7VkObm4lr9SeP39ekrRgwWL5+voV\nW1elShWtW7dWF32GkKdn6ZH2448/6IUX+uuBB1qpefN71KNHb61a9X6p25Z3hDYAXIVq7Ttc02nq\nm2nevFnasmWTXnlluu6/v2WxdY0bN1Vy8ts6e7bAcaS7b99eNWnStESdbt16qF27C+/1xInjGjbs\nBc2YMVf16zeQzXZWkpSenqpatYquk+fknFR+fp5CQkKv6z2EhITK3d1dv/6aq4iIBpKKvhM+ZcoE\nDRgwWHXr1lNW1lHl5+fL399fkvTDDwdKrbVp0wY1a9ZcCQlTHcuOHs1QWFjYdY2xLHAjGgBUIPv3\nf6cPP1ypfv2eV4MGDfXLLzmOfySpefN7FRwcrKlTJyktLVUrVrytlJT9evzxpyQV3Xj2yy85Kiws\nVEBAgEJCQh3/1KgRLMMwVKNGsCpVqqTKlauobdv2Skp6Rfv27dOPP/6gl1+eoObN71X9+iWPsktz\n8Sn33/j6+urxx5/SzJlJ+vbbb3TkSLqmTJmoQ4cOKTQ0TFFRLRQcXFOJiZOVnp6mzz//TFu2bCy1\nVmBgZaWlHVZKyn4dPZqhefNm6/DhQ5e8jl+eEdoAUIFs375Vbm5uWrRogTp37qDOnTvoySdj1Llz\nBxUWFsrd3V2JiTP166+56t8/Vps2bVBi4gwFBxddV/7uu73q3LmDfv659LvJL74RLT5+vJo1u0dD\nhw5VXNxQ3X57Db3yynSnx3txPenC7y++OEz33RethISxev75P8put2n27PmyWCzy9PTUjBlzdebM\nGQ0Y8JxWr16l7t17lvoa3bv3UNOmzRQXN1RDhw6Qp6enhg8fqR9/POj0OMsLN+NSH3NukuzsPJfV\nOnIkTWMWfeWyp3wVnD6hxIHRZf5ozqCgAJf2qSKjV86hT86jV86hT84JCgq4rv050gYAwCQIbQAA\nTILQBgDAJAhtAABMgtAGAMAkCO3LeLB+rpT7V50+8aVL627J2K6x/5iiLRklp9u7lF82btDhPw/T\nLxs3uHQsAADzILQvo02jHLkZ/9Wp486HqzPWp23WKVue1qdtdnqfnI/X6fypX5Xz8eWfeQsAqLgI\n7cvw9ir6CrtRaHNp3bPnbcX+7Yzf5jm+2Q8qAACUH4Q2AAAmQWgDQAVls9n03HPPaPfuXcWWnzx5\nUmPG/Flt2z6obt0e19q1f71snffee0fduj2udu0eUlzci8rKynSsO3PmjBITX1Z0dLQef7ydkpJe\nUUFB2ZwRfOedpXrxxYGSpA0bPlWXLh3LZBw3EqENABWQzWbTpEnjlJ6eVmy5YRiKjx8mm82upUtX\naNCgoZo3b5a++ebrUuv87W+b9M47yxQXN0rLlq2Qr28ljR4d51i/cOEcHTiQoqVLl2r69Dnat2+v\n3nhj7g19b5fz21zmjz7aTsuXv1dm47hReDSnk44cSbvyRlcQGhpW6rNkAcCV0tPTNHnyuFLXffXV\nTv30U6bmzn1T/v7+CgsL1549/9a+fXsUFdWixPZ79/5bUVEt9H//94AkqW/f5/XHP/ZUbm6uqlat\nqr17/60nnuiixo0bKzs7T0891VVr166+oe/PGRaLRRaLpayH4XKEtpPGLPrquva3WXM0M65jmT98\nBEDFt2fPbkVGttCAAYPVps0DxdZ9++1u3XNPlOMZ1JI0YsToS9a6667GWrBgjtLT0xQaWksbNnym\nmjXvUOXKlR3rt2/fqmeffVp5eVZt3/53NWx4V6m1Nmz4VJ988pHuv7+l3n8/WRaLRUOHDpOHh6cW\nLHhdZ86cUefOXTVw4BBJkt1u18KFc7V58+cyjEJFRrbQ8OEjVbVqNUnSkSPpmj79VR04kKJGjZqo\nbt07Ha+1fv0nWrLkTa1Z85kkaefOL7R06SKlp6fJYvHSfffdr1GjJsjX11fLli3WkSPpqlKlijZu\nXC9PTy8988yzio3tI6noEsDcuTO1c+cXys/PU82ad2jgwCF66KFHrvK/zPUjtJ3kqieHAcCN1rlz\nt0uuy8rKVI0awXrrrTe0YcOn8vf319NP91KnTk+Wun2HDp20f/8+xcY+LXd3d1Wq5Kv58xfL3b3o\n6uqwYSP1pz8NUnR0tCSpTp16mjZt5iVf/8CBFIWG1tKSJe/qww9XKinpVTVo0PB/p9b3aObMaWrb\nNkZ169bTm2/OV0rKfs2YMUfe3j5atmyx4uOH6a233pXdbteIES+pWbO7FR8/Vnv37tGsWa+padO7\nJRV/5OexYz9pwoRRiouLV1RUtI4ePaLJk8fro49Wq1evWEnSjh1/V5cuT2vp0hXavv3veuONuWrV\nqrXCw2tr3rxZysg4otdfXygfHx+99947Skp6RS1btpKn582NUUIbAK7CloztWp+2+aq+snm9vD0s\neqxOW7UJe+i6a505Y9Xnn3+mhx9+VNOmzdTBgwc0a1aSqlSpogceKFl//fpP9Le/bdL48ZNVu3Zd\n/fWvKzVu3EgtWZKsgIAAzZ6dpIKCAi1fvlz5+TbNmzdbU6YkKDFxRqmvX1hYqLi4UfLx8dETT3TW\nmjWr1L//INWtW+9/QT1PGRnpCgkJ0dq1H2rRorcVEVFfkjR+/CR17NhG+/btkdWar1OnftXIkWPl\n4+OjsLDa2rNnt7Kzs0u85vnz5/XSSyPUqVNnSVJwcLCiolooPT3VsU1AQKCGDh0mNzc39eoVq/fe\ne1sHDqQoPLy2mjVrru7de6pu3XqSpB49ntWnn67TyZPZCg6ueZ3/Ra4OoQ0AV2Frxo6bGthS0ZwO\nWzN2uCS0PTw8FRAQqPj4cXJzc1P9+g116NAPWrt2damhnZy8XM8+20ft2z8mSRo9eoJ69eqqzz5b\npzZt2mvjxvVauHCpoqOjlZ2dpylTpunpp5/UwYMH1KBBwxL1KleuIh8fH0mSt3fRv2vUCHas9/b2\nls1mV1ZWlux2u4YMGSDDMBzr7Xa7jh7N0KlTpxQSEuKoJUkNGjRSdnbJybBCQ2vJy8tL7767TKmp\nh5WWlqr09FS1adPesU1wcM1iR+e+vn46d+6cJCkmpqN27Nimjz9eq4yMdB08+B9JRR8GbjZCGwCu\nwiNhrcrkSPuRsFYuqXXbbbfJMAqLBVRYWLh27/6m1O2zs3/WnXdeuFbs4eGhO++M0E8/ZSk7+2dJ\n0p13RjjW16x5hwICAvXTT5mlhnZpN+O6uZX8ItNvgbhgwWL5+voVW1elShWtW7dWv8tySbrkqeof\nf/xBL7zQXw880ErNm9+jHj16a9Wq94tt4+XlVWK/3z4sTJkyUfv371P79o/pqae6qVq12zR4cN9S\nX+tGI7QB4Cq0CXvIJUe8ZaVJk2ZasuRNnT9/3hGgaWmpqlmz9NO8ISGhSktL0/33X7ih7ciRI7r7\n7nt0xx0hMgxD6empqlUrSJKUk3NS+fl5CgkJva5xhoSEyt3dXb/+mquIiAaSim4ImzJlggYMGKy6\ndespK+uo8vPzHTfV/fDDgVJrbdq0Qc2aNVdCwlTHsqNHMxQWFnbFcZw5Y9WWLRv15pvL1KhRE0nS\nl1/+47re2/Xge9oAcAt59NF28vT01GuvTdXRoxnauHH9/yYi6S5JOnfunH75JcdxlNm9ew+tWPG2\nvvhimzIzj2ru3Jn69ddfFBPTUZUrV1G7djFKSnpF+/bt048//qCXX56g5s3vVf36JY+yS2NcfLj8\nP76+vnr88ac0c2aSvv32Gx05kq4pUybq0KFDCg0NU1RUCwUH11Ri4mSlp6fp888/05YtG0utFRhY\nWWlph5WSsl9Hj2Zo3rzZOnz4kGw2+xXHZ7F4q1KlStq2bauOHz+mr7/+SvPnvy6p6LvwNxuhDQAV\n2O9Pg0tFYfj66wv1888n1KdPTy1dukh//vNox5H0d9/tVefOHXTixAlJUqdOnTVgwGAtXDhX/fvH\n6vDhQ5o3b7ECA4u+8hUfP17Nmt2joUOHKi5uqG6/vYZeeWX6NY9PuvD7iy8O0333RSshYayef/6P\nstttmj17viwWizw9PTVjxlydOXNGAwY8p9WrV6l7956lvkb37j3UtGkzxcUN1dChA+Tp6anhw0fq\nxx8PXm5kkopOuU+YMEU7dmzTs8920/Llb2nMmAQFB9e8wv43hptxqY85N0l2dp7Lah05kqYxi75y\n2dezJrW/cApk0sYHLrPllRWcPqHEgdEKD6+jIVvjHcsXPJJ0xX2DggK088mujt/rL3n7usZSkQUF\nBbj0z1RFRZ+cR6+cQ5+cExQUcF37c6QNAIBJENoAAJgEoQ0AgEkQ2gAAmAShDQCASRDaAACYBKEN\nAIBJENoAAJgEoQ0AgEkQ2gBQQdlsNj333DPavXtXseXHjx/X8OFD1Lbtg+rd+2l99dU/L1tn1aq/\n6JlnOismprUSEsYoNze31O3eeWepund/wmXjv1rvvLNUL744sMxe/2YgtAGgArLZbJo0aZzS09NK\nrBs9Ok5Vq1bTkiXJiol5TOPHx+v48WOl1vnkk4+0ePEC9e37vBYvfluGIY0Y8acS2x0+fFjvvLNM\nv587vCyUnMu8YiG0AaCCSU9P08CBfXTsWFaJdbt371JmZobi48cpPLy2evfuoyZNmunTT9eVWuuv\nf12pp5/upfbtH1NYWG2NGTNRmZkZ2rXrK8c2hmFo3LhxatSo8Q17TyhCaANABbNnz25FRrbQm28u\nL/Hoy5SU/YqIaCAfHx/HsmbNmmv//n2l1vrppyw1btzU8XulSpVUs2aI9u//zrHsww//Il9fX8XE\nPHbZcW3Y8KleeKG/kpOXq0OHR/TkkzHavPlzbd26RV27dlKHDo9o0aIFju1tNpveeGOeunTpqLZt\nH9SoUcN1/Phxx/ojR9I1dOjzatPmAf3pT4NKnLb/7LOP1bt3dz388P3q1KmNZsyYpsLCwsuOsbwj\ntAGgguncuZuGDh0mb2/vEutyck7qttuCii2rVq2asrN/LrVW1arVlJ19wvF7YWGhTp7M1qlTv0qS\nsrIylZz8tiZPnuzU2A4cSNHRoxlasuRdPfxwGyUlvao1a1Zp+vQ5GjhwiFaseFupqYclSdOnv6od\nO/6uiROnaNGit3X+/HmNHh0nwzBkt9s1YsRLqlEjWMuWrVDbtjFat26143X27dujWbNe08CBQ7Ry\n5VqNHDlW69d/ou3btzo1zvLKs6wHAABmcvrElzp1fLuMQttNe003d4sqBz+kwBr3X3etgoICWSxe\nxZZ5eVlks9lL3f7RR9tpxYp31LRpc4WFhWvZssWyWvNltxdtn5T0qp599jnVqlXLqdcvLCxUXNwo\n+fj46IknOmvNmlXq33+Q6tatp7p16+nNN+cpIyNdQUG3a9OmDZo+fY6aN79XkjRx4lR17drxfzfO\nGTp16leNHDlWPj4+CgurrT17dis7O1uS5O3tozFjJurBB1tLkmrUCFb9+iuUlpaqhx++hsaVE4Q2\nAFyFvJ+/vKmBLUlGoU15P3/pktC2WCyyWq3FltntNvn4lDwql6Q+ffrrxInj6tv3Wbm7u6tt2xhF\nRbWQn5+fPvnkI1mt+XrmmWeLxmmUWqKYypWrOE7Ne3sX/btGjWDHem9vb9lsdh09ekSGYeiuuy5c\nJw8MDFStWuE6ciRNhYWGQkJCip3mb9CgkbKzt//v54by9vbW0qWLlJaWqtTUQ8rKylRUVAsnulR+\ncXocAK5CwO33y83dclNf083dooDbrz+wJSko6Hb98ktOsWU5OTmqXv22Urf39vbWxIlT9Pnn2/Tp\np5s1dmyCfv75ZwUH36HNmz9XauphtWv3kO655x7Nnj1dJ04cU7t2D+nnn0+UWs/Dw6PEMje3klH0\nW6BfrLCw0HFd+uIPCZ6eF45D//WvL9W3b2/98kuOoqP/T1OnJqlJk2al1jQTjrQB4CoE1rjfJUe8\nZaVx46ZKTn5bZ88WOIJx3769atKkaanbL1q0QKGhtdSxY9H3r48d+0np6am6994otWr1sM6eLZAk\nVa/urw8//Ehr136o+fMXl7hufrVCQkLl7u6ulJT9uu++on6fOvWrMjMzFBYWLk9PL2VlHVV+fr78\n/f0lST/8cMCx/yeffKTHHuukESPGSJLOnTunrKxMx6l2syK0AeAW0rz5vQoODtbUqZPUt+/z2rlz\nh1JS9mvMmImSisLt9OlTqlq1mtzc3FS9enUtX/6WateuK29vb02bNkUPPfSIateuU6xuUFCAqlat\nIg8PT91xR4jT47n47vbf+Pj4qHPnrnr99ekaOXKsKleuojfemKvbb6+h++77P7m5uSk4uKYSEydr\nwIAXdOBAirZs2ahGjZpIkipXrqz9+7/T4cOH5Obmpvfee1unT5+W3X5zL224GqfHAaACu3iyEXd3\ndyUmztSvv+aqf/9Ybdq0QYmJMxQcXHRd+bvv9qpz5w46caLo9HaXLk/r4YfbaNSo4Ro2bLAaNmyk\nceMSbtj4fj85y+DBf9J9992viRNH64UX+snHx0dz5rwhLy8veXp6asaMuTpz5owGDHhOq1evUvfu\nPR379u37vKpXv02DBvVVfPww1atXX7GxffTDDwddNvay4GZc6mPOTZKdneeyWkeOpGnMoq/kE1jD\nJfUmtf/HhZ83PnBdtQpOn1DiwGiFh9fRkK3xjuULHkm64r5BQQHa+WRXx+/1l7x9XWOpyIKCAlz6\nZ6qiok/Oo1fOoU/OCQoKuK79OdIGAMAkCG0AAEyC0AYAwCQIbQAATILQBgDAJAhtAABMwqnQ3rt3\nr2JjYyVJ//nPf9SqVSs999xzeu6557RhwwZJ0qpVq9S1a1f16NFD27Ztu2EDBgDgVnXFGdGWLFmi\ndevWyc/PT5K0f/9+9e3bV3369HFsc/LkSSUnJ2vt2rUqKChQz5491bJlS3l5eV2iKgAAuFpXPNIO\nDw/XggUXHkr+/fffa9u2berdu7fGjx8vq9Wqffv2KTIyUp6envL391ft2rV18KC5Z50BAKC8uWJo\nt23btthTWe6++27Fx8drxYoVqlWrlubPn6/8/HwFBFyY5cXX11d5ecyMAwCAK131A0PatGnjCOg2\nbdpo6tSpatGihfLz8x3bWK1WBQYGOlXveqd0+73Tp/1cVutGqFrVr8T7vZb378qeVUT0xzn0yXn0\nyjn06ca76tDu37+/xo8fr6ZNm+rLL79U48aN1bRpU82ePVs2m01nz55VamqqIiIinKrnyrlqc3Ot\nV96oDOXmWku8X2fe/8V/EZjf99KY/9g59Ml59Mo59Mk51/vB5qpDe/LkyZo8ebK8vLwUFBSkl19+\nWX5+foqNjVWvXr1kGIbi4uJksdzch8QDAFDRORXaISEhWrlypSSpYcOG+stf/lJim+7du6t79+6u\nHR0AAHBgchUAAEyC0AYAwCQIbQAATILQBgDAJAhtAABM4qq/8oVrYxiFysrKLLH8yJG0K+578aQx\nv+0TGhpWbLY6AEDFRmjfJDZrrmZ9kCuLX6aqtrmwfMyir5zaf9Lvfh6z6CvZrDmaGddR4eF1XDpO\nAED5RWjfRBa/6vIJrFFs2cW/X9LP17APAKBC4Zo2AAAmQWgDAGAShDYAACZBaAMAYBKENgAAJkFo\nAwBgEoQ2AAAmQWgDAGAShDYAACZBaAMAYBKENgAAJkFoAwBgEoQ2AAAmQWgDAGAShDYAACZBaAMA\nYBKENgAAJkFoAwBgEoQ2AAAmQWgDAGAShDYAACZBaAMAYBKENgAAJkFoAwBgEoQ2AAAmQWgDAGAS\nhDYAACZBaAMAYBKENgAAJkFoAwBgEoQ2AAAmQWgDAGAShDYAACZBaAMAYBKENgAAJkFoAwBgEoQ2\nAAAmQWgDAGAShDYAACZBaAMAYBKENgAAJkFoAwBgEoQ2AAAmQWgDAGAShDYAACZBaAMAYBKENgAA\nJkFoAwBgEoQ2AAAmQWgDAGAShDYAACbhWdYDOHPGKpvN5pJap0+fdkkdAADKozIP7amvL1dWvp9L\nallzs2SpWs8ltQAAKG/KPLS9fSvLwz3EJbXcbeddUgcAgPKIa9oAAJgEoQ0AgEkQ2gAAmAShDQCA\nSRDaAACYBKENAIBJENoAAJiEU6G9d+9excbGSpIyMjLUq1cv9e7dW5MnT3Zss2rVKnXt2lU9evTQ\ntm3bbshgAQC4lV1xcpUlS5Zo3bp18vMrmrUsMTFRcXFxioqKUkJCgrZs2aLmzZsrOTlZa9euVUFB\ngXr27KmWLVvKy8vrhr+BW5VhFCorK9OlNUNDw+Th4eHSmgAA17liaIeHh2vBggWKj4+XJH3//feK\nioqSJLXkpwmWAAANc0lEQVRq1Uo7d+6Uu7u7IiMj5enpKX9/f9WuXVsHDx5UkyZNbuzob2E2a65m\nfZAri59rgttmzdHMuI4KD6/jknoAANe7Ymi3bdtWWVlZjt8Nw3D87Ofnp/z8fFmtVgUEBDiW+/r6\nKi8vz6kBWCyeUsHVDBm/sfhVl09gDZfVq1rVT0FBAVfesJyrCO/hZqBPzqNXzqFPN95Vzz3u7n7h\nMrjValVgYKD8/f2Vn59fYrkzbLZzVzsE3CC5uVZlZzv3Yau8CgoKMP17uBnok/PolXPok3Ou94PN\nVd893qhRI+3atUuStGPHDkVGRqpp06bavXu3bDab8vLylJqaqoiIiOsaGAAAKO6qj7RHjRqlCRMm\nyG63q169eoqJiZGbm5tiY2PVq1cvGYahuLg4WSyWGzFeAABuWU6FdkhIiFauXClJql27tpKTk0ts\n0717d3Xv3t21owMAAA5MrgIAgEkQ2gAAmAShDQCASRDaAACYBKENAIBJXPVXvlAxuXouc+YxBwDX\nI7QhybVzmTOPOQDcGIQ2HFw9lzkAwLW4pg0AgEkQ2gAAmAShDQCASRDaAACYBKENAIBJENoAAJgE\noQ0AgEkQ2gAAmAShDQCASRDaAACYBKENAIBJENoAAJgEoQ0AgEkQ2gAAmAShDQCASRDaAACYBKEN\nAIBJENoAAJgEoQ0AgEkQ2gAAmAShDQCASRDaAACYBKENAIBJENoAAJgEoQ0AgEkQ2gAAmAShDQCA\nSRDaAACYBKENAIBJENoAAJgEoQ0AgEkQ2gAAmAShDQCASRDaAACYBKENAIBJENoAAJgEoQ0AgEkQ\n2gAAmAShDQCASRDaAACYBKENAIBJENoAAJgEoQ0AgEkQ2gAAmAShDQCASRDaAACYBKENAIBJENoA\nAJgEoQ0AgEkQ2gAAmAShDQCASRDaAACYBKENAIBJENoAAJgEoQ0AgEkQ2gAAmAShDQCASRDaAACY\nhGdZDwAVj2EUKisr06U1Q0PD5OHh4dKaAGA2hDZczmbN1awPcmXxc01w26w5mhnXUeHhdVxSDwDM\n6ppDu0uXLvL395ckhYaGatCgQRo9erTc3d0VERGhhIQElw0S5mPxqy6fwBplPQwAqFCuKbRtNpsk\n6d1333UsGzx4sOLi4hQVFaWEhARt2bJFbdq0cc0oAQDAtd2IduDAAZ05c0b9+vVTnz59tHfvXqWk\npCgqKkqS1KpVK3355ZcuHSgAALe6azrS9vHxUb9+/dS9e3elp6drwIABMgzDsd7Pz095eXkuGyQA\nALjG0K5du7bCw8MdP1epUkUpKSmO9VarVYGBgU7Vslg8pYJrGQVuJVWr+ikoKMCpbZ3d7lZHn5xH\nr5xDn268awrtNWvW6ODBg0pISNCJEyeUn5+vli1b6uuvv1aLFi20Y8cORUdHO1XLZjt3LUPALSY3\n16rs7CufvQkKCnBqu1sdfXIevXIOfXLO9X6wuabQ7tatm8aOHatnn31Wbm5umjZtmqpUqaLx48fL\nbrerXr16iomJua6BAQCA4q4ptD09PZWUlFRieXJy8nUPCAAAlI5pTAEAMAlCGwAAkyC0AQAwCUIb\nAACTILQBADAJQhsAAJMgtAEAMAlCGwAAkyC0AQAwCUIbAACTuKZpTIGbyTAKlZWV6dS2p0/7KTfX\nesXtQkPD5OHhcb1DA4CbitBGuWez5mrWB7my+DkX3Feul6OZcR0VHl7HJfUA4GYhtGEKFr/q8gms\n4ZJaV3Pk7gyO2gHcLIQ2bjmuPHLnqB3AzURo45bkyiN3ALhZuHscAACTILQBADAJQhsAAJMgtAEA\nMAlCGwAAkyC0AQAwCUIbAACTILQBADAJQhsAAJMgtAEAMAlCGwAAkyC0AQAwCUIbAACTILQBADAJ\nQhsAAJMgtAEAMAlCGwAAkyC0AQAwCUIbAACTILQBADAJQhsAAJMgtAEAMAlCGwAAk/As6wEAZmYY\nhcrKynRpzdDQMHl4eLi0JoCKgdAGroPNmqtZH+TK4uea4LZZczQzrqPCw+u4pB6AioXQBq6Txa+6\nfAJrlPUwANwCuKYNAIBJENoAAJgEoQ0AgEkQ2gAAmAShDQCASRDaAACYBKENAIBJENoAAJgEoQ0A\ngEkQ2gAAmAShDQCASRDaAACYBKENAIBJENoAAJgEoQ0AgEnwPG2gHDGMQmVlZbq0ZmhomDw8PFxa\nE0DZILSBcsRmzdWsD3Jl8XNNcNusOZoZ11Hh4XVcUg9A2SK0gXLG4lddPoE1ynoYAMohrmkDAGAS\nHGkDFdilrpGfPu2n3FzrVdfj+jhQtghtoAJz5TXys/nZ+nOPexQSEuqCkRXhQwBwdQhtoIJz1TXy\ns/k5mvXBXm6SA8oQoQ3AadwkB5QtbkQDAMAkCG0AAEyC0AYAwCQIbQAATILQBgDAJFx697hhGJo0\naZIOHjwoi8WiV155RbVq1XLlSwCoIFz9cBS+841bgUtDe8uWLbLZbFq5cqX27t2rxMRELVy40JUv\nAaCCcOXEL67+zvf58+eVmZkh6dpnj7u4nuQmD4/rP7npylq/ceUHnt/3zhX4MFacS0N79+7devDB\nByVJd999t/bv3+/K8gAqmPL6ve/MzAz9edZnsvhVd0m9/OxDsvhWdUk9V9aSXP+Bx5W9YwKeklwa\n2vn5+QoICLhQ3NNThYWFcne/9CdCw3ZaFusZl7y+peCYCuy3uaTWxQpOn7iu/e3/zZWbW9HPla6z\nbsHpE8XquYIr65Xnsbm6Xnkem6vrleex2aw5Lj3V7upnmpd3rni/v52RuNV6d7O5GYZhuKrYtGnT\n1Lx5c8XExEiSWrdurW3btrmqPAAAtzSX3j1+7733avv27ZKkPXv2qH79+q4sDwDALc2lR9q/v3tc\nkhITE1WnDtciAABwBZeGNgAAuHGYXAUAAJMgtAEAMAlCGwAAkyC0AQAwCZdOruIs5ii/tHPnzmns\n2LHKysqS3W7XoEGDdOedd2r06NFyd3dXRESEEhISynqY5UZOTo66du2q5cuXy8PDgz5dwuLFi7V1\n61adO3dOvXv31r333kuvLmIYhsaNG6e0tDR5eHhoypQp/Jkqxd69ezVjxgwlJycrIyOj1P6sWrVK\nH3zwgby8vDRo0CC1bt26bAddBn7fp//85z+aOnWqPDw8ZLFYlJSUpGrVql1bn4wysGnTJmP06NGG\nYRjGnj17jMGDB5fFMMql1atXG6+++qphGIZx6tQpo3Xr1sagQYOMXbt2GYZhGBMnTjQ2b95clkMs\nN+x2uzFkyBCjffv2RmpqKn26hH/961/GoEGDDMMwDKvVasyZM4delWLHjh3GsGHDDMMwjJ07dxov\nvvgifbrIW2+9ZXTq1Ml45plnDMMwSu1Pdna20alTJ8Nutxt5eXlGp06dDJvNVpbDvuku7lPv3r2N\nAwcOGIZhGCtXrjSmTZt2zX0qk9PjzFF+aR06dNBLL70kqWjifQ8PD6WkpCgqKkqS1KpVK3355Zdl\nOcRy47XXXlPPnj11++23yzAM+nQJ//jHP1S/fn298MILGjx4sB555BF6VQpvb2/l5eXJMAzl5eXJ\n09OTPl0kPDxcCxYscPz+/fffF+vPP//5T+3bt0+RkZHy9PSUv7+/ateu7Zi741ZxcZ9mz56tBg0a\nSCo6m2qxWK65T2US2peaoxxSpUqV5Ovrq/z8fL300ksaPny4jN99ld7Pz095eXllOMLyYc2aNape\nvbpatmzp6M/v/wzRpwtyc3O1f/9+zZ07V5MmTdKIESPoVSkiIyN19uxZxcTEaOLEiYqNjeXv3kXa\ntm1b7IlbF/cnPz9fVqu12P/ffX19b7m+Xdyn224reibGt99+q/fff199+vQpkYPO9qlMrmn7+/vL\nar3wqLsrPVTkVnPs2DENHTpUvXv3VseOHTV9+nTHOqvVqsDAwDIcXfmwZs0aubm5aefOnTp48KBG\njRql3Nxcx3r6dEGVKlVUr149eXp6qk6dOvL29taJExceVEOviixZskT33nuvhg8frhMnTig2NlZ2\nu92xnj6V9Pv/b//WH39/f+Xn55dYfqtbv369Fi1apMWLF6tq1arX3KcySUrmKL+0kydPql+/fho5\ncqSeeuopSdJdd92lXbt2SZJ27NihyMjIshxiubBixQolJycrOTlZDRs2VFJSkh588EH6VIrIyEh9\n8cUXkqQTJ07ov//9r6Kjo/X1119Lole/OXPmjPz9/SVJAQEBOnfunBo1akSfLqNRo0Yl/s41bdpU\nu3fvls1mU15enlJTUxUREVHGIy1b69at03vvvafk5GSFhIRIkpo1a3ZNfSqTI+22bdtq586d6tGj\nh6SiOcpRZNGiRTp9+rQWLlyoBQsWyM3NTePGjdPUqVNlt9tVr149x1PUUNyoUaM0YcIE+nSR1q1b\n65tvvlG3bt0c39wICQnR+PHj6dXv9OvXT2PGjFGvXr10/vx5jRgxQo0bN6ZPl1Ha3zk3NzfFxsaq\nV69eMgxDcXFxslgsZT3UMlNYWKhXX31Vd9xxh4YMGSI3Nze1aNFCQ4cOvaY+Mfc4AAAmwYVkAABM\ngtAGAMAkCG0AAEyC0AYAwCQIbQAATILQBgDAJAhtAABM4v8BAOgL3V2/jrMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2ae9435c50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# x ~ N(0, 2)\n",
    "x = np.random.gamma(2, 10, size=1000)\n",
    "plt.hist(x, bins=20);\n",
    "media = np.mean(x)\n",
    "mediana = np.median(x)\n",
    "\n",
    "# Las siguientes líneas sirven para computar la moda, la razón de computar la moda de esta forma y el significado de estas lineas será explicado más adelante.\n",
    "kde = stats.gaussian_kde(x)\n",
    "vec = np.linspace(np.min(x), np.max(x), 1000)\n",
    "moda = vec[np.argmax(kde.evaluate(vec))]\n",
    "# Ahora agregaremos lineas verticales para indicar las medidas de centralidad\n",
    "plt.axvline(media, ymax=.9, c='r', lw='3', label='{:.2f} media'.format(media))\n",
    "plt.axvline(mediana, ymax=.9, c='g', lw='3', label='{:.2f} mediana'.format(mediana))\n",
    "plt.axvline(moda, 0, ymax=.9, c='y', lw='3', label='{:.2f} moda'.format(moda))\n",
    "plt.legend(fontsize=14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Calcular la moda a partir de una lista (o array) de flotantes no es tan simple. La razón es que en general no encontraremos dos números exactamente iguales. Existe una función en SciPy que permite obtener la moda de una lista (o array), pero no nos será de utilidad. Como se puede ver a continuación el computo de la moda no devuelve (en general) el valor esperado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'moda 0.362'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'moda {:.3f}'.format(stats.mode(x)[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2 Dispersión"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.2.1 Varianza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mide la dispersión de un conjunto de valores. Es cero para un conjunto de valores idénticos.\n",
    "\n",
    "$$V(X) = E[(X - \\mu)^2] = \\int_{-\\infty}^{\\infty} (x-\\mu)^2 d(x) = \\frac{1}{n} \\sum_{i=1}^n (x_i - \\mu)^2$$\n",
    "\n",
    "Donde $\\mu$ es la media de $X$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.2.2 Desviación estándar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es simplemente la raiz cuadrada de la varianza, en muchos problemas teóricos resulta más facil manipular varianzas que desviaciones estándar, pero en general resulta más simple interpretar las desviaciones estándar ya que las unidades son las mismas que la de los datos.\n",
    "\n",
    "$$\\sigma = \\sqrt{V(X)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "varianza 213.25\n",
      "desviación estándar 14.60\n"
     ]
    }
   ],
   "source": [
    "print('varianza {:.2f}'.format(np.var(x)))\n",
    "print('desviación estándar {:.2f}'.format(np.std(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-5-de59bb90eeea>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-5-de59bb90eeea>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    Existen otras medidas para caraterizar los datos, llamadas de forma, como son la [curtosis](https://en.wikipedia.org/wiki/Kurtosis) y el [sesgo](https://en.wikipedia.org/wiki/Skewness) (o asimetría estadística).\u001b[0m\n\u001b[1;37m                ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Existen otras medidas para caraterizar los datos, llamadas de forma, como son la [curtosis](https://en.wikipedia.org/wiki/Kurtosis) y el [sesgo](https://en.wikipedia.org/wiki/Skewness) (o asimetría estadística).\n",
    "\n",
    "Estás medidas son menos usadas en parte por que su interpretación es menos intuitiva que otras medidas como la media o la varianza, al punto que la interpretación correcta de estas medidas ha sido objeto de varias discusiones y malos entendidos a los largo de los años. Otra razón para su menor uso es que historicamente gran parte de la estadística se ha basado en el uso de Gausianas (o en asumir que los datos son Gaussianos) para las cuales la curtosis y el sesgo son cero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.3 Cuantil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los cuantiles son puntos de corte que dividen al conjunto de datos en grupos de igual tamaño. Existen varios tipos de cuantiles, de acuerdo a la cantidad de divisiones que nos interesen.\n",
    "\n",
    "* Los cuartiles son los tres puntos que dividen a la distributición en 4 partes iguales, se corresponden con los cuantiles 0.25, 0.50 y 0.75.\n",
    "* Los quintiles dividen a la distribución en cinco partes (corresponden a los cuantiles 0.20, 0.40, 0.60 y 0.80);\n",
    "* Los deciles, que dividen a la distribución en diez partes.\n",
    "* Los percentiles, que dividen a la distribución en cien partes.\n",
    "* La mediana es el percentil 50 o el cuartil 0.5.\n",
    "    \n",
    "En Python el cálculo de estos estadísticos puede realizarse facilmente usando funciones predefinidas en NumPy y SciPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('percentiles 25={:.2f}; 50={:.2f}; 75={:.2f}'.format(*(np.percentile(x , [25, 50, 75]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todas las medidas antes mencionadas son útiles por que resumen en pocos números una gran cantidad de datos. Sin embargo, al sintetizar la información, también pueden ocultarla. Es por ello que siempre es buena idea visualizar la distribución de los datos. Cuatro representaciones comunes son los histogramas, _kernel plots_, _box plots_ y _violin plots_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.4 Histogramas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En un [histograma](https://en.wikipedia.org/wiki/Histogram) se representa la frecuencia con la que aparecen los distintos valores en un conjunto de datos. Se utilizan _barras_ contiguas para representar los datos. La superficie (y no la altura) de las barras es proporcional a la frecuencia de datos observados. Los datos son agrupados en _bins_, y suelen graficarse sin normalizar o normalizados. Normalizar implica que la superficie total del histograma suma 1. No hay que confundir los histogramas con los gráficos de barras que se utilizan para comparar valores discretos entre grupos y mientras que los histogramas se usan para representar distribuciones.\n",
    "\n",
    "Los histogramas son sensibles a la cantidad de _bins_ que se usan. Si usamos unos pocos _bins_ no lograremos capturar la estructura de los datos, si usamos demasiados _bins_ no solo estaremos representando la estructura de los datos si no también el ruido. Esto se ve más claramente si nos vamos a los extremos, por un lado tendríamos un sola barra, por el otro una barra por cada dato."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.hist(x, bins=50, normed=True, cumulative=False);  # probá cambiar los bins, y los demás argumentos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.5 Kernel Density plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es una forma _suavizada_ de un histograma. Un gráfico _[KDE](https://en.wikipedia.org/wiki/Kernel_density_estimation)_ se dibuja de la siguiente forma: se reemplaza cada dato por una distribución Gaussiana y luego se suman todas las Gaussianas. En vez de una distribución Gaussiana es posible usar otras distribuciones. El nombre genérico para esas distribuciones cuya suma se usa como aproximación de una función es el de _kernel_. Cualquier función simétrica cuya integral sea 1 puede ser usada como kernel, la Gaussiana es una de los kernels más usados.\n",
    "\n",
    "De forma análoga a lo que sucede con los _bins_ los KDE son sensibles a un parámetro llamado _bandwith_. Existen métodos automatizados para ajustar el _bandwith_ de forma automática de acuerdo a los datos, los que en general funcionan bastante bien."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.kdeplot(x);   #también ver la función sns.distplot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usando SciPy podemos obtener la kde y trabajar con ella. En el siguiente ejemplo estimamos primero la kde de x, luego evaluamos la función entre -6 y 6 y por último computamos la moda, como el valor para el cual la densidad toma el valor máximo (el valor mas frecuente de la distribución)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kde = stats.gaussian_kde(x)\n",
    "vec = np.linspace(-6,6, 1000)\n",
    "moda = vec[np.argmax(kde.evaluate(vec))]\n",
    "print('moda {:.2f}'.format(moda))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.6 Gráficos de cajas o de bigotes (Box plot o Wisker-plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los [gráficos de caja](https://en.wikipedia.org/wiki/Box_plot) son una forma de graficar distribuciones basada en cuartiles. Es múy util cuando se quieren comparar varios grupos de datos en simultaneo. La caja está delimitada por el primer y tercer cuartil, mientrás que la linea dentro de la caja es el segundo cuartil (la mediana). Los _bigotes_ pueden indicar varias medidas, por eso es simpre importante leer/escribir la leyenda o texto que acompaña a un boxplot, a veces se usa una desviación estandard, otras veces los percentiles 2 y 98, otras veces (como en el gráfico a continuación) es una función del rango intercuartil y los valores por fuera de los bigotes se suelen considerar como datos aberrantes (ver más adelante)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = np.random.normal(-1, 10, size=200)\n",
    "sns.boxplot(data=[x, y]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.7 Gráficos de violín (violin plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los gráficos de [violín](https://en.wikipedia.org/wiki/Violin_plot) son una combinación de gráficos de caja con kde."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.violinplot(data=[x, y]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.8 Datos aberrantes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los datos aberrantes (_outliers_) son valores que están muy alejados de la mayoría de los valores de una distribución. Los datos aberrantes pueden ser errores de medición, errores al procesar los datos o incluso valores correctos pero inusuales (sobre todo cuando la muestra es pequeña). Siempre es buena idea revisar si nuestros datos contiene datos aberrantes y en muchos casos puede llegar a ser conveniente removerlos. Siempre que se remueve un dato aberrante deberá reportarse que fue removido y explicar cual fue el criterio usado para removerlos. Es importante destacar que la decisión de remover datos aberrantes no debe ser tomada a la ligera. Si un supuesto dato _aberrante_ fuese un valor correcto quizá nos estaría indicando que no comprendemos del todo el fenómeno estudiado y al dejarlo de lado podríamos estar perdiendonos de un importante descubrimiento!\n",
    "\n",
    "Existen varios criterios para identificar datos aberrantes. Dos muy usados son:\n",
    "   * todo valor por debajo de $\\mu-n\\sigma$ y por encima de $\\mu+n\\sigma$. Donde n = 1, 2, 3, 6 etc...\n",
    "   * Se define el rango intercuartil como $IQR = q2 − q3 = p25-p75$ y se define como _aberrante_ todo valor por debajo de $q2-1.5*IQR$ y por encima de $q3+1.5*IQR$\n",
    "\n",
    "El primer criterio suele ser usado para distribuciones que se asemejan a Gaussianas mientras que el segundo es más general ya que el rango intercuartil es una medida más robusta de la dispersión de una distribución que la desviación estándar.\n",
    "\n",
    "Según la desigualdad de Chebyshev, al menos  $1 - \\frac{1}{k^2}$ de los valores de una distribución están dentro $k$ desviaciones estandard. Es decir casi todos los valores de una distribución de probabilidad están cerca de la media. Por lo tanto el 75% y el 89% de los valores de una distribución se encuentran dentro de 2 y 3 desviaciones estandard, respectivamente. La desigualdad de Chebyshev indica una cota, para varias distribuciones es posible que los valores se encuentren mucho más concentrados alrededor de la media. Por ejemplo esto sucede con las curvas Gaussianas. Para una curva Gaussiana se cumple la regla 68-95-99,7 es decir el 68 por cierto de los datos se encuentra dentro de 1 desviación estandard, el 95 dentro de 2 y el 99.7 dentro de 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Relación entre dos variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los gráficos que hasta ahora hemos visto sirven para visualizar una variable por vez, (aunque _sns.kde()_ soporta la visualización de dos varibles). En muchos casos necesitamos entender la relación entre dos variables. Dos variables están inter-relacionadas, si el conocer el valor de una provee de información sobre el valor de la otra."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.1 Gráfico de dispersión"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un [gráfico de dispersión](https://en.wikipedia.org/wiki/Scatter_plot) es un gráfico científico que utiliza las coordenadas cartesianas para mostrar los valores de dos variables en simultaneo. Estos gráficos son la forma más simple de visualizar la relación entre dos variables.\n",
    "\n",
    "Supongamos que tenemos dos variables, que creativamente llamaremos $x$, $y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = np.random.normal(size=1000)\n",
    "y = np.random.normal(loc=x, scale=1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usando matplotlib podemos graficar ambas variables usando la funcion _scatter_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(x, y)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seaborn provee de multiples opciones para visualizar relaciones entre dos variables, varias de ellas están contenidas en la función _joinplot_. Esta función además de mostrar el gráfico de dispersión muestra las distribuciones marginales de $x$ e $y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.jointplot(x, y,  kind='scatter', stat_func=None);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente gráfico usa un _kernel density estimation_ como vimos anteriormente, lo hace tanto para las distribuciones marginales como la distribución conjunta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.jointplot(x, y,  kind='kde', stat_func=None);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un gráfico de _hexbin_ es similar a un histograma, pero bidimensional. El nombre se debe a que los datos son agrupados en celdas hexagonales. Por qué hexágonos en vez de cuadrados o triángulos? Simplemente por que las celdas hexagonales introducen una menor distorsión en los datos que otras opciones. Esto se debe a las siguientes razones:\n",
    "\n",
    "* Los hexágonos se relacionan con sus vecinos por lados (los cuadrados y triángulos lo hacen por vértices y lados). Es decir se vinculan con sus vecinos de forma más simétrica. \n",
    "* Los hexágonos son el polígono con mayor número de lados que mejor cubren ([teselan]()) una superficie plana.\n",
    "* Los hexágonos introducen menor distorsión visual que por ejemplo los cuadrados. Un malla cuadrada nos hace que tendamos a mirar en sentido horizontal y vertical.\n",
    "\n",
    "Los hexbin son útiles cuando necesitamos visualizar muchos datos. Por muchos me refiero a números por encima de las centenas de miles de datos. Cuando tenemos una gran cantidad de datos los puntos empiezan a superponerse y puede que ciertos patrones pasen desapercibidos, por lo que visualizar datos agrupados y no los datos _crudos_ suele ser buena idea. Además calcular los _hexbin_ tiene un costo menor que los KDE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.jointplot(x, y,  kind='hex', stat_func=None);  # ver también plt.hexbin();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una alternativa para evitar que algunos puntos opaquen al resto, en un grafico de dispersión _común_, es hacer los puntos semitrasparentes. En matplotlib la trasparencia de los objetos es controlada mediante un parámetro llamado _alpha_ que va entre 0 y 1. Este es un buen momento para volver algunas celdas atrás y ver como este y otros parámetros pueden ser usados para modificar las gráficas realizadas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.2 Correlación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como dijimos en la sección anterior dos variables están relacionadas si uno provee información sobre la otra. Siendo un poco más formales decimos que dos variables son independientes entre si, si se cumple que:\n",
    "\n",
    "$$p(x|y) = p(x)$$\n",
    "\n",
    "Es decir si la probabilidad de $x$ dado $y$ es igual a la probabilidad de $x$. En otras palabras, tener información de $y$ no me brinda información sobre $x$. Si dos variables son independientes entonces:\n",
    "\n",
    "$p(x,y) = p(x) p(y)$\n",
    "\n",
    "La probabilidad de que ocurra $x$ e $y$ de forma simultanea es igual al producto de sus probabilidades.\n",
    "\n",
    "La correlación es una medida de la dependencia de dos variables.  Existen varios coeficientes de correlación el más frecuentemente usado es el coeficiente de correlación de Pearson. Este coeficiente solo sirve para medir relaciones lineales entre variables. El coeficiente de correlación de Pearson es el resultado de dividir la covarianza de las dos variables por el producto de sus desviaciones estándar:\n",
    "\n",
    "$$\\rho_{(X,Y)}={E[(X-\\mu_X)(Y-\\mu_Y)] \\over \\sigma_X\\sigma_Y}$$\n",
    "\n",
    "En palabras (que puede ser más oscuro que en fórmulas), el coeficiente de correlación de Pearson indica como varia una variable al variar la otra respecto de la variación intrínseca de cada una de las variables.\n",
    "\n",
    "Por que usar el coeficiente de Pearson y no directamente la covarianza? Principalmente por que las unidades no suelen tener sentido físico y la magnitud suele ser dificil de interpretar, por ejmeplo el número que indica una relación lineal perfecta será disinto dependiendo de los datos usados. En cambio al dividir por las desviaciones estándar, obtenemos una cantidad que varía entre -1 y 1 y que no tiene dimensiones. \n",
    "\n",
    "La función _joinplot_, que vimos en el apartado anterior, por defecto nos devuelve el valor del coefficiente de correlación de Person, junto con un valor $p$ cuyo significado estudiaremos en el capítulo sobre estadística frecuentista."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.jointplot(x, y,  kind='scatter');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identificar correlaciones puede ser útil para entender como dos variables se relacionan y para predecir una a partir de la otra. Es por ello que muchas veces además de visualizar la relación entre variables se estiman modelos que _ajustan_ a los datos. Como por ejemplo lineas rectas. En los próximos capítulos veremos como crear modelos lineales y no-lineales. Por ahora simplemente nos conformaremos con dejar que seaborn ajuste los datos a un recta por nosotros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.jointplot(x, y,  kind='reg');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la siguiente imágen se puede ver varios conjuntos de datos y sus respectivos coeficientes de correlación de Pearson. Es importante notar que el coefficiente de correlacion de Pearson refleja la linearidad y la dirección de dicha linearidad (primera fila), pero no la pendiente de dicha relación (fila del medio). Tampoco es capaz de capturar relaciones no-lineales. En la fila del medio la linea con pendiente cero tiene un coefficiente de correlación de Pearson indefinido, ya que la varianza de la variable $y$ es 0.\n",
    "\n",
    "<img src='imagenes/Correlación.png' alt=\"correlación\", width=600, height=600> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.3 Correlación y causalidad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si existe algún tipo de mecanismo que hace que una variable dependa de otra deberá existir correlación (aunque no necesariamente lineal). Pero lo opuesto no es siempre cierto, dos variables pueden estar correlacionadas sin que exista ningún tipo de mecanismo que las vincule. Dado el gran conjunto de variables que es posible medir no debería ser sorprendente que existan correlaciones [espurias](http://www.tylervigen.com/). Por ejemplo en la siguiente figura se puede ver que el número piratas y la media de la temperatura global están inversamente correlacionados.\n",
    "\n",
    "<img src='http://upload.wikimedia.org/wikipedia/commons/thumb/d/de/PiratesVsTemp%28en%29.svg/1024px-PiratesVsTemp%28en%29.svg.png' alt=\"Pirates_temp\", width=600, height=600> \n",
    "\n",
    "Este gráfico fue creado a propósito para ilustrar, entre otros puntos, que correlación no implica causalidad (notesé además que el orden de los datos en el eje x es erroneo y la escala es al menos _problemática_). Para más detalles del origen de esta gráfica leer esta entrada de [wikipedia](https://es.wikipedia.org/wiki/Pastafarismo#Los_piratas_y_el_calentamiento_global) \n",
    "\n",
    "La aparente relación entre las variables temperatura media y cantidad de piratas podría ser explicada de varias formas, quizá es pura casualidad o quizá se podría establecer que los cambios introducidos por la revolución industrial terminaron por un lado por un aumentando la cantidad de $CO_2$ (y otros gases de invernadero) y por el otro produciendo cambios socio-culturales y tecnológicos que llevaron (luego de una larga cadena de sucesos) a la disminución de los piratas. Pero no es cierto que podamos contrarrestar el calentamiento global simplemente aumentando la cantidad de piratas!\n",
    "\n",
    "Para poder establecer una relación causal a partir de una correlación hace falta poder establecer y probar la existencia de un mecanismo que vincule ambas variables. Espero que este ejemplo haya servido para ayudarles a entender que correlación no implica  causalidad. \n",
    "\n",
    "<img src='http://imgs.xkcd.com/comics/correlation.png' alt=\"xkcd\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Estadística Inferencial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En muchos casos describir los datos y graficarlos puede ser suficiente para nuestros proósitos. Pero en otros casos esto es solo el primer paso. El siguiente paso consiste en realizar un análisis que permita comprender los procesos o mecanismos que dieron origen a los datos observados. Esto se conoce como estadística inferencial y es una forma de modelado. \n",
    "\n",
    "Los modelos son una descripción del sistema bajo estudio y no solo una descripcion de los datos! Los modelos son simplificaciones de procesos más complejos, lo importante es que capturen lo mejor posible la información relevante, por lo que un modelo más complejo no necesariamente es mejor que uno simple. Cuando decimos que un conjunto de datos obtenidos son explicados por un modelo estadístico estamos simplificando el problema. Decenas, cientos o miles de datos pueden pasar a ser descriptos por unos pocos parámetros que definen al modelo (en cierto modo estamos comprimiendo información). Por ejemplo podemos decir que la relación entre cantidad de lluvia ($x$) y crecimiento de una planta ($y$) podría (quizá) ser descripto por la ecuación de una recta:\n",
    "\n",
    "$$y = \\alpha + \\beta * x  + \\epsilon$$\n",
    "\n",
    "En este caso nuestro modelo sería una linea recta y tendría tres parámetros $\\alpha$, $\\beta$ y $\\epsilon$. Algunas de las ventajas de modelar son:\n",
    "\n",
    "* Los modelos, al ser más simple que el fenómeno estudiado, permiten pensar el problema con mayor claridad.\n",
    "* Crear un modelo implica pensar acerca del problema y evaluar cuales son los factores que se consideran más relevantes y cuales pueden ser despreciados. \n",
    "* Los modelos estadísticos (como otros modelos formales) pueden ser estudiados analíticamente y/o numéricamente lo que puede contribuir enormemente a comprender el fenómeno subyacente a los datos.\n",
    "\n",
    "En los últimos años se ha desarrollado un conjunto de técnicas agrupadas bajo el rotulo de _machine learning_. El _machine learning_ es algo así como una reinvención de la estadística por parte de informáticos más preocupados en resolver problemas que de matemáticos preocupados en demostrar teoremas.\n",
    "Una de las diferencias (ha tomar con pinzas) entre la estadística inferencial y el _machine learning_ es el énfasis en el modelo. El _machine learning_ usa modelos de caja negra es decir los parámetros del modelo no son de interés principal, lo importante es predecir. En cambio la estadística inferencial pone enfasis en entender los parámetros del modelo como medio para entender el problema y enventualmente predecir el comportamiento del sistema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5.1 Los tres objetivos de la inferencia estadística"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Siguiendo el ejemplo de la linea recta a continuación vamos a describir brevemente los tres objetivos de la inferencia estadística. Luego, a medida que avancemos en el curso, veremos cada uno de ellos en detalle.\n",
    "\n",
    "* **Estimación de parámetros**: En este caso lo que nos interesa es estimar los valores más probables de los parámetros ($\\alpha$, $\\beta$ y $\\epsilon$) y el grado de incerteza o error con que estamos estimando dichos parámetros. \n",
    "\n",
    "\n",
    "* **Predicción de valores**: En otros casos lo que nos interesa no tiene tanto que ver con los valores que pueden tomar los parámetros si no que estamos focalizados en obtener un modelo que nos permita, a partir de un nuevo conjunto de datos $x_{n}$, predecir el comportamiento del sistema (representada por la variable $y$).\n",
    "\n",
    "\n",
    "* **Comparación/Elección de modelos**: Por último en este caso el énfasis está puesto en comparar distintos modelos entre si a fin de determinar cual es mejor explicando los datos observados. Después de todo podría ser que la cantidad de agua y el crecimiento de una planta tuvieran una relación que no fuese necesariamente lineal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Probabilidad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las probabilidades son números entre 0 y 1 (incluyendo ambos extremos) y que cumplen con las dos siguientes reglas.\n",
    "\n",
    "La regla de la suma:\n",
    "$$p(A) + p (\\bar A) = 1$$ \n",
    "\n",
    "Esto se lee como, _la probabilidad de A más la probabilidad de no-A es igual a 1_. Es decir siempre algo sucede.\n",
    "\n",
    "La regla del producto:\n",
    "$$p(A, B) = p(A|B) \\times p(B)$$ \n",
    "\n",
    "Esto se lee como la _probabilidad de que A y B ocurran en simultaneo es la probabilidad de A dado B por la probabilidad de B_. $p(A|B)$ es lo que se conoce como probabilidad condicional, y es la probabilidad de que ocurra A condicionada por el hecho que sabemos que B ha ocurrido. Estrictamente todas las probabilidades son condicionales (respecto de algún supuesto o modelo) aún cuando no lo expresemos explicitamente. Podríamos decir que no existen probabilidades sin contexto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6.1 Probabilidad clásica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La probabilidad clásica, a veces también llamada _naive_, asume que todos los eventos son igualmente probables. Puede ser util para calcular la probabilidad de tirar un dado y obtener el número 2 ($\\frac{1}{6}$). Pero puede llevar a errores si pretendemos usar la misma lógica para computar la probabilidad de vida en marte. Por ejemplo podríamos asumir que o bien hay o bien no hay vida en marte y por lo tanto llegar a la erronea estimación de que la probabilidad de vida en marte es de 0.5!\n",
    "\n",
    "$$ P(X=x) = \\frac{\\text{# eventos favorables}}{\\text{# total de eventos}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6.2 Probabilidad frecuentista"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La probabilidad clásica es empírica. En vez de asumir que los eventos son igualmente probables diseñamos un experimentos (en el sentido muy amplio de la palabra) y contamos cuantas veces vemos el evento que nos interesa respecto del total de intentos. Esta definición de probabilidad nos permitiría estimar que la probabilidad de obtener 2 al arrojar un dado 100 veces es de por ejemplo $\\frac{1}{3}$. Notese que la probabilidad solo sería $\\frac{1}{6}$, si el dado no está cargado, si el experimento está bien hecho y si el número de intentos es infinito (a los fines prácticos y para este problema _infinito_ pueder ser unos pocos miles de intentos). La definicion frecuentista de probabilidad tiene el problema que no es muy util para pensar en problemas que ocurren una sola vez. Por ejemplo, ¿Cuál es la probabilidad que mañana llueva? Estrictamente solo hay un mañana y o bien lloverá o no. Los frecuentistas suelen evadir este problema recurriendo a experimentos imaginarios. En ese caso podríamos intentar estimar la probabilidad de lluvia para mañana imaginando que hay _infinitos mañanas_ y _contar_ en cuantos de esos _mañanas_ llueve  y en cuantos no.\n",
    "\n",
    "$$p(X=x) = \\lim_{n \\rightarrow \\infty} \\frac{\\text{# veces que el evento ocurrió}}{\\text{# de intentos identicos e independientes}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6.3 Probabilidad Bayesiana"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En estadística Bayesiana las probabilidades son usadas para cuantificar la confianza que tenemos de que un evento ocurra. Desde este punto de vista es totalmente razonable preguntar cual es la probabilidad de que la masa de Saturno sea X, o hablar sobre la probabilidad de lluvia durante el 25 de Mayo de 1810, o la probabilidad de que mañana amanezca. La probabilidad frecuentista y la clásica, son casos especiales de la probabilidad Bayesiana."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6.4 Distribuciones de probabilidad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En estadística suele ser útil pensar que existe una distribución $h(x)$ a partir de la cual \"se generan\" los valores observados de $x$. La idea central de la estadística Bayesiana es invertir el proceso y estimar la distribución $h(x)$ a partir de los valores observados de $x$. La necesidad de realizar una _inferencia_ se debe a que en general $h(x)$, es una función que no puede ser observada directamente. Por lo tanto lo mejor que podemos hacer es construir un modelo $g(x)$ que sea capaz de explicar a $x$. La sospecha y esperanza (al menos de los _[realistas](http://es.wikipedia.org/wiki/Realismo_cient%C3%ADfico)_) es que mientras mejor explique $g(x)$ a $x$ mas parecida será $g(x)$. Nuestro modelo, $g(x)$, podría se por ejemplo una distribución Gaussiana, el modelo no está completo si no encontramos además los parámetros de esa distribución (que en este caso son dos, la media y desviación standard).\n",
    "\n",
    "Cuando $x$ puede tomar valores continuos, la función $g(x)$, es llamada función de densidad de probabilidad ($pdf$ por su sigla en inglés) y cuando $x$ solo puede tomar valores discretos, $g(x)$ es llamada función de masa de probabilidad ($pmf$ por su sigla en inglés). La $pdf$ cuantifica la probabilidad de que un valor se encuentre entre $x$ y $x + dx$ (expresado como $g(x)dx$). A $x$ se la llama variable aleatoria, notese que _aleatoria_ no quiere decir que $x$ pueda tomar cualquier valor, de hecho solo puede tomar los valores definidos por $g(x)$.\n",
    "\n",
    "A modo de ejemplo veamos como luce la $pdf$ de una distribución Gaussiana:\n",
    "\n",
    "$$\n",
    "g(x) = \\frac{1}{\\sigma \\sqrt{ 2 \\pi}} e^{ - \\frac{ (x - \\mu)^2 } {2 \\sigma^2}}\n",
    "$$\n",
    "\n",
    "Como puede verse, una distribución Gaussiana queda definida por dos parámetros:\n",
    "\n",
    "$x$, la media de la distribución  \n",
    "$\\sigma$, la desviación standard  \n",
    "\n",
    "Donde $x \\in \\mathbf{R}$ y $\\sigma > 0$. \n",
    "Todas las posibles curvas que resulten de la combinación de esos dos parámetros serán curvas de la familia de las Gaussianas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_values = np.linspace(-4, 4, 100)\n",
    "gauss0 = stats.norm(0, .2).pdf(x_values)\n",
    "gauss1 = stats.norm(0, 1).pdf(x_values)\n",
    "gauss2 = stats.norm(0, 2).pdf(x_values)\n",
    "gauss3 = stats.norm(-2, .5).pdf(x_values)\n",
    "\n",
    "plt.plot(x_values, gauss0, lw=3, color='b',)\n",
    "plt.plot(x_values, gauss1, lw=3, color='g')\n",
    "plt.plot(x_values, gauss2, lw=3, color='r')\n",
    "plt.plot(x_values, gauss3, lw=3, color='m')\n",
    "plt.xlabel('$x$', fontsize=18);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La integral de la $pdf$ es llamada función de distribución acumulada ($cdf$):\n",
    "\n",
    "\\begin{equation}\n",
    "cdf(x) = \\int_{-\\infty}^{x} h(x) d(x)\n",
    "\\end{equation}\n",
    "\n",
    "En algunas situaciones se prefiere hablar de la función de supervivencia:\n",
    "\n",
    "\\begin{equation}\n",
    "S(x) = 1 - cdf \n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gauss0 = stats.norm(0, .2).cdf(x_values)\n",
    "gauss1 = stats.norm(0, 1).cdf(x_values)\n",
    "gauss2 = stats.norm(0, 2).cdf(x_values)\n",
    "gauss3 = stats.norm(-2, .5).cdf(x_values)\n",
    "\n",
    "plt.plot(x_values, gauss0, lw=3, color='b')\n",
    "plt.plot(x_values, gauss1, lw=3, color='g')\n",
    "plt.plot(x_values, gauss2, lw=3, color='r')\n",
    "plt.plot(x_values, gauss3, lw=3, color='m')\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('CDF');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6.5 Incerteza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algo a tener en cuenta es que **siempre** $x$ tiene asociado un error o incerteza $e(x)$ (salvo que los datos sean sintéticos, o usted esté hablando con un _matemático puro_). Este error puede ser definido como la probabilidad de medir un valor $x$ dado que el valor _real_ es $\\mu$:\n",
    "\n",
    "$$\n",
    "er(x) = p(x|\\mu, I)\n",
    "$$\n",
    "\n",
    "Donde $I$ es algún parámetro (o conjunto de parámetros) de alguna distribución de probabilidad.\n",
    "\n",
    "En muchos casos se asume que la distribución de errores es Gaussiana. Este supuesto a veces está justificado por nuestro _conocimiento específico de dominio_ y en otros casos por conveniencia matemática (las Gaussianas tienen varias propiedades que las hacen amenas para el análisis matemático).\n",
    "\n",
    "$$\n",
    "er(x|\\mu,\\sigma) = \\frac{1}{\\sigma \\sqrt{ 2 \\pi}} e^{ - \\frac{ (x - \\mu)^2 } {2 \\sigma^2}}\n",
    "$$\n",
    "\n",
    "En este caso $I$ es simplemente $\\sigma$. La función de distribución del error puede además contener un error sistemático $b$ en cuyo caso $x-\\mu$ en la expresión de arriba se convierte en $(x − b − \\mu)$.\n",
    "\n",
    "Cuando la distribución de los errores es la misma para todos los puntos se dice que hay _homocedasticidad_, caso contrario occurre _heterocedasticidad_. Muchos de los métodos de la estadística clásica más comunes son válidos solo cuando la distribución de errores es homocedástica, existiendo métodos especiales para lidiar con la _heterocedasticidad_.\n",
    "\n",
    "Es importante notar que en la práctica $f(x)$ describe tanto la variabilidad intrínseca de una (o múltiples) variable(s) en una población como también el error asociado a la medición."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6.6 Variables aleatorias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una variable aleatoria es una variable que proviene de alguna distribución de probabilidad dada, en la práctica es el resultado de una observación, experimento o simulación. En la literatura estadística se suelen escribir usando letras mayúsculas como $X$, mientras que los valores particulares (instancias) de esa variable se escriben usando la misma letra pero en minúscula por ej $x$. Esta notación tiene mucho sentido si se piensa a $X$ como un vector.\n",
    "\n",
    "Existen dos principales tipos de variables aleatorias, las continuas y las discretas.\n",
    "\n",
    "En muchos análisis estadísticos se asume que las variables aleatorias provienen de una misma distribución y que son independientes entre si. En estos casos se dice que dichas variables son independientes e idénticamente distribuidas (en ingles iid), es decir:\n",
    "\n",
    "$p(x,y) = p(x) p(y)$ para todos los valores de $x$ e $y$.\n",
    "\n",
    "Si una variable aleatoria $X$ proviene o está _distribuida_ según una distribución normal con media $\\mu$ y desviación standard $\\sigma$ se suele escribir como:\n",
    "\n",
    "$$\n",
    "X \\sim \\mathcal{N} (\\mu, \\sigma)\n",
    "$$\n",
    "\n",
    "Un caso común de variable aleatoria que no es iid son las series temporales (y las cadenas de Markov) ya que existe una dependencia temporal de los valores que puede tomar una variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6.7 Distribuciones de probabilidad comunes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los bloques con los cuales se construyen los modelos estadísticos son las distribucciones de probabilidad. A continuación veremos algunas de las más comunes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.6.7.1 Distribución uniforme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aún siendo simple, la distribución [uniforme](https://en.wikipedia.org/wiki/Uniform_distribution_%28continuous%29) es muy usada en estadística, por ej para representar nuestra ignorancia sobre el valor que pueda tomar un parámetro. La distribución uniforme tiene entropía cero (todos los estados son igualmente probables).\n",
    "\n",
    "$$\n",
    "p(x|a,b)=\\begin{cases} \\frac{1}{b-a} & para\\ a \\le x \\le b \\\\ 0 &  para\\ x<a\\ o\\ x>b \\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "distri = stats.uniform(0, 1)  # distribución uniforme entre a=0 y b=1\n",
    "x_values = np.linspace(-0.5, 1.5, 100)\n",
    "x = distri.rvs(500)  # muestrear 500 valores de la distribución\n",
    "x_pdf = distri.pdf(x_values)  # la pdf evaluada para todos los x_values\n",
    "plt.hist(x, normed=True)\n",
    "plt.plot (x_values, x_pdf, lw=4, color='r')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('p(x)')\n",
    "\n",
    "mean, var = distri.stats(moments='mv')\n",
    "print(mean, var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.6.7.2 Distribución Gaussiana (o normal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es quizá la [distribución](https://en.wikipedia.org/wiki/Normal_distribution) más conocida. Por un lado por que muchos fenomenos pueden ser descriptos (aproximadamente) usando esta distribución. Por otro lado por que posee ciertas propiedades matemáticas que facilitan trabajar con ella de forma analítica. Es por ello que muchos de los resultados de la estadística frecuentista se basan en asumir una distribución Gaussiana. Por ejemplo el método de los mínimos cuadrados para ajustar modelos lineales no es válido cuando los errores medidos se desvían de una distribución normal.\n",
    "\n",
    "La distribución Gaussiana se define usando dos parámetros, la media $\\mu$ y la desviacion estándar $\\sigma$. Una distribucion Gaussiana con $\\mu = 0$ y $\\sigma = 1$ es conocida como la _distribución Gaussiana estándar_.\n",
    "\n",
    "$$\n",
    "p(x|\\mu,\\sigma) = \\frac{1}{\\sigma \\sqrt{ 2 \\pi}} e^{ - \\frac{ (x - \\mu)^2 } {2 \\sigma^2}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "distri = stats.norm(loc=0, scale=1)  # media cero y desviación standard 1\n",
    "x_values = np.linspace(-4, 4, 100)\n",
    "x = distri.rvs(500)  # muestrear 500 valores de la distribución\n",
    "x_pdf = distri.pdf(x_values)  # la pdf evaluada para todos los x_values\n",
    "plt.hist(x, normed=True)\n",
    "plt.plot (x_values, x_pdf, lw=4, color='r')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('p(x)')\n",
    "\n",
    "mean, var = distri.stats(moments='mv')\n",
    "print(mean, var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.6.7.3 Distribución t de Student"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Historicamente esta [distribución](http://es.wikipedia.org/wiki/Distribuci%C3%B3n_t_de_Student) surgió para estimar la media de una población normalmente distribuida cuando el tamaño de la muestra es pequeño. En estadística Beyesiana su uso más frecuente es el de generar modelos robustos a datos aberrantes, como veremos mas adelante. \n",
    "\n",
    "\n",
    "$$p(x\\mid \\nu,\\mu,\\sigma) = \\frac{\\Gamma(\\frac{\\nu + 1}{2})}{\\Gamma(\\frac{\\nu}{2})\\sqrt{\\pi\\nu}\\sigma} \\left(1+\\frac{1}{\\nu}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2\\right)^{-\\frac{\\nu+1}{2}}\n",
    "$$\n",
    "\n",
    "donde $\\Gamma$ es la función [gamma](https://en.wikipedia.org/wiki/Gamma_function) y donde $\\nu$ es un parámetro llamado _grados de libertad_ (nombre que tiene más sentido en un contexto frecuentista que en uno bayesiano), como veremos a este parámetro le podríamos llamar _grado de normalidad_, ya que a medida que aumenta la distribución se aproxima a una Gaussiana. En el caso extremo de $\\nu = \\inf$ la distribución es exactamente igual a una Gaussina.\n",
    "\n",
    "En el otro _extremo_, cuando $\\nu=1$, (aunque en realidad $\\nu$ puede tomar valores por debajo de 1) estamos frente a una distribución de [Cauchy](https://en.wikipedia.org/wiki/Cauchy_distribution). Es similar a una Gaussiana pero las colas decrecen muy lentamente, eso provoca que en teoría esta distribución no poseen una media o varianza definidas. Es decir, es posible calcular a partir de un conjunto de datos una media, pero si los datos provienen de una distribución de cauchy, la dispersión alrededor de la media será alta y esta dispersión no disminuirá a medida que aumente el tamaño de la muestra. La razón de este comportamiento extraño es que en distribuciones como la Cauchy están dominadas por lo que sucede en las colas de la distribución, contrario a lo que sucede por ejemplo con la distribución Gaussiana.\n",
    "\n",
    "Para esta distribución $\\sigma$ no es la desviación estandard, que como ya se dijo podría ser infinita o estar indefinida, $\\sigma$ es la _escala_. A medida que $\\nu$ aumenta la _escala_ convergue a la desviación estandard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "distri = stats.t(loc=0, scale=2, df=4) # media 0, sscala 2, grados de libertad 4\n",
    "x_values = np.linspace(-10, 10, 100)\n",
    "x = distri.rvs(500)  # muestrear 500 valores de la distribución\n",
    "x_pdf = distri.pdf(x_values)  # la pdf evaluada para todos los x_values\n",
    "plt.hist(x, normed=True)\n",
    "plt.plot (x_values, x_pdf, lw=4, color='r')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('p(x)')\n",
    "\n",
    "mean, var = distri.stats(moments='mv')\n",
    "print(mean, var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.6.7.4 Distribución exponencial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La distribución [exponencial](https://en.wikipedia.org/wiki/Exponential_distribution) se define solo para $x > 0$. Esta distribución se suele usar para describir el tiempo que transcurre entre dos eventos que ocurren de forma continua e independiente a una taza fija. El número de tales eventos para un tiempo fijo lo da la distribución de Poisson (ver más adelante). Nosotros también la usaremos para fijar los _a prioris_ de $nu$ (los grados de libertad de distribucion t de Student).\n",
    "\n",
    "$$\n",
    "p(x|\\lambda) = \\lambda e^{-\\lambda x}\n",
    "$$\n",
    "\n",
    "La media y la desviación estandard de esta distribucion están dadas por $\\frac{1}{\\lambda}$ \n",
    "\n",
    "Scipy usa una parametrización  diferente donde la escala es igual a $\\frac{1}{\\lambda}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "distri = stats.expon(scale=3)  # escala 3, lambda = 1/3\n",
    "x_values = np.linspace(0, 25, 1000)\n",
    "x = distri.rvs(500)  # muestrear 500 valores de la distribución\n",
    "x_pdf = distri.pdf(x_values)  # la pdf evaluada para todos los x_values\n",
    "plt.hist(x, bins=25, normed=True)\n",
    "plt.plot (x_values, x_pdf, lw=4, color='r')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('p(x)')\n",
    "\n",
    "mean, var = distri.stats(moments='mv')\n",
    "print(mean, var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.6.7.5 Distribución beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es una [distribución](https://en.wikipedia.org/wiki/Beta_distribution) definida en el intervalo [0, 1]. Se usa para modelar el comportamiento de variables aleatorias limitadas a un intervalo finito. Es util para modelar proporciones o porcentajes. \n",
    "\n",
    "$$\n",
    "p(x|\\alpha, \\beta)= \\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}\\, x^{\\alpha-1}(1-x)^{\\beta-1}\n",
    "$$\n",
    "\n",
    "El primer término es simplemente una costante de normalización que asegura que la integral de la $pdf$ de 1. $\\Gamma$ es la función [gamma](https://en.wikipedia.org/wiki/Gamma_function). Cuando $\\alpha=1$ y $\\beta=1$ la distribución beta se reduce a la distribución uniforme.\n",
    "\n",
    "Si queremos expresar la distribución beta en función de la media y la dispersión alrededor de la media podemos hacerlo de la siguiente forma.\n",
    "\n",
    "$$\\alpha = \\mu \\kappa$$\n",
    "$$\\beta = (1 − \\mu) \\kappa$$\n",
    "\n",
    "Siendo $\\mu$ la media y $\\kappa$ una parámetro llamado concentración a media que $\\kappa$ aumenta la dispersión disminuye. Notese, además que $\\kappa = \\alpha + \\beta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "distri = stats.beta(5, 2)  # alfa=5, beta=2\n",
    "x_values = np.linspace(0, 1, 100)\n",
    "x = distri.rvs(500)  # muestrear 500 valores de la distribución\n",
    "x_pdf = distri.pdf(x_values)  # la pdf evaluada para todos los x_values\n",
    "plt.hist(x, normed=True)\n",
    "plt.plot (x_values, x_pdf, lw=4, color='r')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('p(x)')\n",
    "\n",
    "mean, var = distri.stats(moments='mv')\n",
    "print(mean, var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.6.7.6 Distribución Gamma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scipy parametriza a la distribución [gamma](https://en.wikipedia.org/wiki/Gamma_distribution) usando un parámetro $\\alpha$ y uno $\\theta$, usando estos parámetros la $pdf$ es:\n",
    "\n",
    "$$\n",
    "p(x \\mid \\alpha, \\theta) = \\frac{1}{\\Gamma(\\alpha) \\theta^\\alpha} x^{\\alpha \\,-\\, 1} e^{-\\frac{x}{\\theta}}\n",
    "$$\n",
    "\n",
    "Una parametrización más común en estadística Bayesiana usa los parámetros $\\alpha$ y  $\\beta$, siendo $\\beta = \\frac{1}{\\theta}$. Esta parametrización es usada por ejemplo por PyMC3. En este caso la pdf queda como:\n",
    "\n",
    "$$\n",
    "p(x \\mid \\alpha, \\beta) = \\frac{\\beta^{\\alpha}x^{\\alpha-1}e^{-\\beta x}}{\\Gamma(\\alpha)}\n",
    "$$\n",
    "\n",
    "Esta distribución la usaremos principalmente para fijar _a prioris_ para las desviaciones standard (o en general escalas) de otras distribuciones. En muchos casos puede resultar mas útil o intuitivo expresar una distribución gama en función de la media y desviación estandard, siguiendo las siguientes relaciones.\n",
    "\n",
    "$\\alpha = \\frac{\\mu^2}{\\sigma^2}$;\n",
    "$\\beta = \\frac{\\mu}{\\sigma^2}$;\n",
    "$\\theta = \\frac{\\sigma^2}{\\mu}$\n",
    "\n",
    "Como veremos, PyMC3 permite expresar la distribución gama directamente usando la media y desviación estandard, sin necesidad de tener que convertir parámetros usando las relaciones anteriores.\n",
    "\n",
    "La distribución gamma se reduce a la exponencial cuando $\\alpha=1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "distri = stats.gamma(a=3, scale=0.5)  # alfa 3, theta 0.5\n",
    "x_values = np.linspace(0, 8, 100)\n",
    "x = distri.rvs(500)  # muestrear 500 valores de la distribución\n",
    "x_pdf = distri.pdf(x_values)  # la pdf evaluada para todos los x_values\n",
    "plt.hist(x, normed=True)\n",
    "plt.plot (x_values, x_pdf, lw=4, color='r')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('p(x)')\n",
    "\n",
    "mean, var = distri.stats(moments='mv')\n",
    "print(mean, var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.6.7.7 Distribución binomial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es la [distribución](https://en.wikipedia.org/wiki/Binomial_distribution) de probabilidad discreta que cuenta el número de éxitos en una secuencia de $n$ ensayos de Bernoulli (experimentos si/no) independientes entre sí, con una probabilidad fija $\\theta$ de ocurrencia del éxito entre los ensayos.\n",
    "Cuando $n=1$ esta distribución se reduce a distribución de Bernoulli.\n",
    "\n",
    "$$p(x|n,\\theta)=\\frac{n!}{x!(n-x)!}p^x(1-p)^{n-x}$$\n",
    "\n",
    "El término $p^x(1-p)^{n-x}$ indica la probabilidad de obtener $x$ éxitos en $n$ intentos. Este término solo tiene en cuenta el número total de éxitos obtenidos pero no la secuencia en la que aparecieron. El primer término conocido como **coeficiente binomial** calcula todas las posibles combinaciones de $n$ en $x$, es decir el número de subconjuntos de $x$ elementos escogidos de un conjunto con n elementos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "distri = stats.binom(n=4, p=0.5)  # n=número de intentos, p=probabilidad del evento \"1\"\n",
    "x_values = np.arange(0, 5)\n",
    "x_pdf = distri.pmf(x_values)  # la pdf evaluada para todos los x_values\n",
    "plt.vlines(x_values, 0, x_pdf, colors='r', lw=5)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('p(x)')\n",
    "\n",
    "mean, var = distri.stats(moments='mv')\n",
    "print(mean, var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.6.7.8 Distribución de Poisson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es una [distribución](https://en.wikipedia.org/wiki/Poisson_distribution) de probabilidad discreta que expresa, a partir de una frecuencia de ocurrencia media $\\mu$, la probabilidad de que ocurra un determinado número de eventos $k$ durante un cierto intervalo de tiempo (o espacio, volumen, etc). Concretamente, se especializa en la probabilidad de sucesos con probabilidades muy pequeñas (sucesos _raros_).\n",
    "\n",
    "$$\n",
    "p(k|\\mu) = \\frac{\\mu^{k} e^{-\\mu}}{k!}\n",
    "$$\n",
    "\n",
    "La media de esta distribución está dada por $\\mu$ y la desviación estardad por $\\sqrt{\\mu}$. A medida que $\\mu$ aumenta la distribución de Poisson se aproxima a una distribución Gaussiana."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "distri = stats.poisson(2.3)  # occurrencia media del evento\n",
    "x_values = np.arange(0, 10)\n",
    "x_pdf = distri.pmf(x_values)  # la pdf evaluada para todos los x_values\n",
    "plt.vlines(x_values, 0, x_pdf, colors='r', lw=5)\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('p(k)')\n",
    "\n",
    "mean, var = distri.stats(moments='mv')\n",
    "print(mean, var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7 Probability plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un método gráfico para comparar si un conjunto de datos se ajusta a una distribución teórica es usar los _probability plots_ en los cuales se representan en el eje $x$ los cuantiles de la distribución teórica y en el eje $y$ los valores de los datos ordenados de menor a mayor. Si la distribución empírica fuese exactamente igual a la teórica los puntos caerían sobre la linea recta a $45^{\\circ}$, es decir la linea donde $y = x$.\n",
    "\n",
    "SciPy provee una función para realizar este gráfico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "muestra = np.random.randn(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 7))\n",
    "plt.subplot(2, 2, 1)\n",
    "stats.probplot(muestra, plot=plt)\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "stats.probplot(muestra, plot=plt, dist=stats.beta(0.5, 0.5))\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "x = np.linspace(-4, 4, 100)\n",
    "plt.plot(x, stats.norm(0, 1).pdf(x))\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "x = np.linspace(0, 1, 100)\n",
    "plt.plot(x, stats.beta(0.5, 0.5).pdf(x))\n",
    "\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.8 ¿Por qué Normal?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La distribución Gaussiana (o normal) aprece una y otra vez en teoría y en la práctica estadística. Esto ocurre principalmente por dos razones.\n",
    "\n",
    "1. El teorema del límite central garantiza que la distribución Gaussiana surja (casi) siempre que calculemos promedios. \n",
    "1. Las distribuciones Gaussianas son, matemáticamente, simples de manipular. Por lo que historicamente han sido la base de la mayoría de los desarrollos en estadística."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.9 El teorema del límite central"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El teorema del límite central dice que si tomamos $n$ valores (de forma independiente) de una distribucion arbitraria, la media de esa variables sigue una distribución Gaussiana cuando $lim_{n\\to\\infty}$. Es decir aun cuando no sepamos nada sobre la distribución que genera los números podemos asegurar que la suma (o el promedio) de esas variables seguirá, de forma aproximada, una distribución Gaussiana.\n",
    "\n",
    "Este resultado puede parecer sorprendente, pero tiene una explicación simple. Al tomar valores de una distribución los valores altos se compensan (en promedio) con los valores bajos, por lo que no es sorprendente que obtengamos una distribución suave y con un solo pico. Por supuesto este experimento mental no nos dice que la distribución es una Gaussiana, para confirmar esto hay que sentarse y resolver la matemática (o hacer una simulación como haremos brevemente)\n",
    "\n",
    "Para que el teorema del límite central se cumpla se deben cumplir los siguientes supuestos.\n",
    "\n",
    "1. Las variables se meustrean de forma independiente\n",
    "1. Las variables provienen de la misma distribución\n",
    "1. La media y la desviación estándar de la distribución tiene que ser finita\n",
    "\n",
    "Los criterios 1 y 2 se pueden relajar _bastante_ y aún así obtendremos aproximadamente una Gaussiana, pero del criterio 3 no hay forma de escapar. Para distribuciones como la distribución de Cauchy o Paretto, que no posen varianza definida este teorema no se aplica. El promedio de $N$ valores provenientes de una distribución Cauchy no siguen una Gaussiana sino una distribucion de Cauchy.\n",
    "\n",
    "El teorema del límite central explica, al menos en parte, la prevalencia de la distribución Gaussiana en la naturaleza. Muchos de los fenómenos que estudiamos responden a la combinación de múltiples causas combinadas. Por ejemplo, un fenotipo dado es el resultado de un grán número de factores genéticos y ambientales interactuando entre si.\n",
    "\n",
    " Además, el error de esa aproximación decrese como $\\sim \\sqrt n$ y no con $n$). Es decir para lograr una reducción del error del 10% necesitamos aumentar la cantidad de datos en dos ordenes de magnitud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,6))\n",
    "iters = 1000\n",
    "for i, n in enumerate([1, 10, 100]):\n",
    "    sample = [np.sum(np.random.exponential(5, size=n)) for _ in range(iters)]\n",
    "    plt.subplot(2, 3, i+1)\n",
    "    sns.kdeplot(np.array(sample), label=n)\n",
    "    plt.subplot(2, 3, i+4)\n",
    "    stats.probplot(sample, plot=plt, fit=None)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.10 La ley de los grandes números (el casino siempre gana)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El valor promedio calculado para una muestra converge al valor esperado (media) de dicha distribución. Al igual que el teorema del límite central esto no es cierto para la distribución de Cauchy (la cual no tiene media finita).\n",
    "\n",
    "La ley de los grandes números se suele malinterpretar y dar lugar a la paradoja del apostador. Un ejemplo de esta paradoja es creer que conviene apostar en la lotería/quiniela a un número _atrasado_, es decir un número que hace tiempo que no sale. El razonamiento, erroneo, es que como todos los números tienen la misma probabilidad a largo plazo (según la ley de los grande números) si un número viene _atrasado_ entonces hay alguna especie de fuerza que aumenta la probabilidad de ese número en el próximo sorteo para así re-establecer la equiprobabilidad de los números. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tamaño_muestra = 200\n",
    "N_samples = range(1, tamaño_muestra)\n",
    "expected_value = 0.5\n",
    "\n",
    "for _ in range(3):\n",
    "    muestra = np.random.uniform(0, 1, tamaño_muestra)\n",
    "    promedio = [muestra[:i].mean() for i in N_samples]\n",
    "    plt.plot(N_samples, promedio, lw=1.5)\n",
    "\n",
    "plt.hlines(expected_value, 0, tamaño_muestra, linestyle='--', color='k')\n",
    "plt.ylabel(\"Promedio de las muestras\", fontsize=14)\n",
    "plt.xlabel(\"# de muestras\", fontsize=14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.11 Z-score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es una cantidad adimensional que expresa que indica el número de desviaciones estándar que un dato está por encima o por debajo de la media. Cuando el Z-score es positivo el dato está por encima de la media y cuando es negativo está por debajo de la media. Se calcula como\n",
    "\n",
    "$$z = \\frac{x - \\mu}{\\sigma}$$\n",
    "\n",
    "Donde:\n",
    "\n",
    "$\\mu$ es la media de la población\n",
    "$\\sigma$ es la desviación estándard de la población\n",
    "\n",
    "El proceo de restar la media y dividir por la desviación estandar se llama normalización o estandarización."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.12 Error estándard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El error estándar es la desviación estándar de alguna medida estimada, por lo general la media (aunque podría ser cualquier otra cantidad).\n",
    "\n",
    "Si tomamos un conjunto de datos y calculamos la media de esos datos y luego tomamos otra muestra y calculamos la media y luego otra y otra, obtendremos que los valores de la media no son siempre los mismos. Si tomamos todas esas medias obtendremos una distribución de medias con una media y desviación estándar esa desviación estándar será el error estandar de la media. El error estándar de la media se suele estimar como: \n",
    "\n",
    "$$\\frac{\\sigma}{\\sqrt(n)}$$\n",
    "\n",
    "donde $\\sigma$ es la desviación estándar de los datos y $n$ la cantidad de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.13 Para seguir leyendo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [wikipedia :-)](http://en.wikipedia.org)\n",
    "* [Think Stats](http://greenteapress.com/thinkstats/)\n",
    "* [Data Analysis with Open Source Tools](http://shop.oreilly.com/product/9780596802363.do)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys, IPython, scipy, matplotlib, platform\n",
    "print(\"Esta notebook fue creada en una computadora %s corriendo %s y usando:\\nPython %s\\nIPython %s\\nNumPy %s\\nSciPy %s\\nMatplotlib %s\\nSeaborn %s\\n\" % (platform.machine(), ' '.join(platform.linux_distribution()[:2]), sys.version[:5], IPython.__version__, np.__version__, scipy.__version__, matplotlib.__version__, sns.__version__))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
